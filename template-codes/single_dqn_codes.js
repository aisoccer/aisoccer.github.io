// from: https://github.com/lfelipesv/test_world_3d/blob/blockcode-examples/examples/player_deeplearning-single-dqn_py

var ALGORITHM_DQN = 
'#!/usr/bin/python3\n'
+ '\n'
+ '# Author(s): Luiz Felipe Vecchietti, Kyujin Choi, Taeyoung Kim\n'
+ '# Maintainer: Kyujin Choi (nav3549@kaist.ac.kr)\n'
+ '\n'
+ 'from networks import Agent\n'
+ 'import torch\n'
+ 'import torch.nn as nn\n'
+ 'import torch.nn.functional as F\n'
+ 'import torch.optim as optim\n'
+ 'from copy import deepcopy\n'
+ 'import numpy as np\n'
+ '\n'
+ 'import helper\n'
+ 'import os\n'
+ 'import json\n'
+ 'from episode_memory import Memory\n'
+ 'import random\n'
+ '\n'
+ 'CHECKPOINT= os.path.join(os.path.dirname(__file__), \'policy.pt\')\n'
+ '\n'
+ 'def update_target_model(net, target_net):\n'
+ '    target_net.load_state_dict(net.state_dict())\n'
+ '\n'
+ 'class DQN:\n'
+ '    def __init__(self, n_agents, dim_obs, dim_act, load=False, play=False):\n'
+ '        params_file = open(os.path.dirname(__file__) + \'/parameters.json\')\n'
+ '        params = json.loads(params_file.read())\n'
+ '        self.n_agents = n_agents\n'
+ '        self._iterations = 0\n'
+ '        self.update_steps = 100 # Update Target Network\n'
+ '        self.epsilon_steps = params[\'dqn_parameters\'][\'epsilon_steps\'] # Decrease epsilon\n'
+ '        self.play = play\n'
+ '        if self.play == True:\n'
+ '            self.epsilon = 0 # Greedy choice if play is True\n'
+ '        else:\n'
+ '            self.epsilon = 1.0 # Initial epsilon value      \n'
+ '        self.final_epsilon = 0.05 # Final epsilon value\n'
+ '        self.dec_epsilon =  0.025 # Decrease rate of epsilon for every generation\n'
+ '\n'
+ '        self.observation_steps = 1000 # Number of iterations to observe before training every generation\n'
+ '        self.save_num = 5000 # Save checkpoint # default: 100\n'
+ '        self.batch_size = params[\'dqn_parameters\'][\'batch_size\']\n'
+ '\n'
+ '        self.num_inputs = dim_obs\n'
+ '        self.act_size = dim_act\n'
+ '        self.memory = Memory(params[\'dqn_parameters\'][\'buffer_size\']) # replay buffer\n'
+ '        self.net = Agent(self.num_inputs, self.act_size)\n'
+ '        self.target_net = Agent(self.num_inputs, self.act_size)\n'
+ '        self.load = load\n'
+ '        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n'
+ '        if self.load == True:\n'
+ '            self.net.load_state_dict(torch.load(CHECKPOINT, map_location=torch.device(self.device)))\n'
+ '            helper.printConsole("loading variables...")\n'
+ '        \n'
+ '        update_target_model(self.net, self.target_net)\n'
+ '        self.net.train()\n'
+ '        self.target_net.train()\n'
+ '        self.net.to(self.device)\n'
+ '        self.target_net.to(self.device)\n'
+ '        self.gamma = params[\'dqn_parameters\'][\'gamma\']\n'
+ '        self.grad_norm_clip = params[\'dqn_parameters\'][\'grad_norm_clip\']\n'
+ '        self.loss = 0\n'
+ '        self.lr = params[\'dqn_parameters\'][\'lr\']\n'
+ '        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)    \n'
+ '\n'
+ '    def select_action(self, state):\n'
+ '\n'
+ '        state = torch.Tensor(state).to(self.device)\n'
+ '        \n'
+ '        qvalue = self.net(state)\n'
+ '        qvalue = qvalue.cpu().data.numpy()\n'
+ '        \n'
+ '        pick_random = int(np.random.rand() <= self.epsilon)\n'
+ '        random_actions = random.randrange(self.act_size)\n'
+ '        picked_actions = pick_random * random_actions + (1 - pick_random) * np.argmax(qvalue)\n'
+ '        return picked_actions\n'
+ '\n'
+ '    def store_experience(self, state, next_state, act, rew):\n'
+ '        # Store transition in the replay buffer.\n'
+ '        self.memory.push(state, next_state, act, rew)\n'
+ '\n'
+ '    def update_policy(self):\n'
+ '\n'
+ '        batch = self.memory.sample(self.batch_size)\n'
+ '\n'
+ '        states = torch.Tensor(batch.state).to(self.device)\n'
+ '        next_states = torch.Tensor(batch.next_state).to(self.device)\n'
+ '        actions = torch.Tensor(batch.action).long().to(self.device)\n'
+ '        rewards = torch.Tensor(batch.reward).to(self.device)\n'
+ '\n'
+ '        q_values = self.net(states).squeeze(1)\n'
+ '        max_next_q_values = self.target_net(next_states).squeeze(1).max(1)[0]\n'
+ '\n'
+ '        one_hot_action = torch.zeros(self.batch_size, q_values.size(-1)).to(self.device)\n'
+ '        one_hot_action.scatter_(1, actions.unsqueeze(1), 1)\n'
+ '        chosen_q_values = torch.sum(q_values.mul(one_hot_action), dim=1)\n'
+ '\n'
+ '        target = rewards + self.gamma * max_next_q_values\n'
+ '\n'
+ '        td_error = (chosen_q_values - target.detach())\n'
+ '        loss = (td_error ** 2).sum() \n'
+ '        self.loss = loss.cpu().data.numpy()\n'
+ '        \n'
+ '        self.optimizer.zero_grad()\n'
+ '        loss.backward()\n'
+ '        grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), self.grad_norm_clip)\n'
+ '        self.optimizer.step()\n'
+ '        \n'
+ '        if self._iterations  % self.update_steps == 0: \n'
+ '            update_target_model(self.net, self.target_net)\n'
+ '            helper.printConsole("Updated target model.")\n'
+ '\n'
+ '        if self._iterations  % self.epsilon_steps == 0: \n'
+ '            self.epsilon = max(self.epsilon - self.dec_epsilon, self.final_epsilon)\n'
+ '            helper.printConsole("New Episode! New Epsilon:" + str(self.epsilon))\n'
+ '\n'
+ '        return self.loss        \n'
+ '\n'
+ '    def save_checkpoint(self, iteration):\n'
+ '        if iteration % self.save_num ==0:\n'
+ '            self.net.save_model(self.net, CHECKPOINT)\n'
+ '            helper.printConsole("Saved Checkpoint.")\n'
+ '\n'
+ '    def print_loss(self, loss, iteration):\n'
+ '        if self._iterations % 100 == 0: # Print information every 100 iterations\n'
+ '            helper.printConsole("======================================================")\n'
+ '            helper.printConsole("Epsilon: " + str(self.epsilon))\n'
+ '            helper.printConsole("iterations: " + str(self._iterations))\n'
+ '            helper.printConsole("Loss: " + str(loss))\n'
+ '            helper.printConsole("======================================================")\n'
+ '\n'
+ '    def update(self):\n'
+ '        if len(self.memory) > self.observation_steps:\n'
+ '            self._iterations += 1\n'
+ '            loss = self.update_policy()\n'
+ '            self.print_loss(loss, self._iterations)\n';

var EPISODE_MEMORY_DQN = 
'#!/usr/bin/python3\n'
+ '\n'
+ '# Author(s): Luiz Felipe Vecchietti, Kyujin Choi, Taeyoung Kim\n'
+ '# Maintainer: Kyujin Choi (nav3549@kaist.ac.kr)\n'
+ '\n'
+ 'import random\n'
+ 'from collections import namedtuple\n'
+ '\n'
+ 'Transition = namedtuple(\'Transition\', (\'state\', \'next_state\', \'action\', \'reward\'))\n'
+ '\n'
+ 'class Memory(object):\n'
+ '    def __init__(self, capacity):\n'
+ '        self.memory = []\n'
+ '        self.capacity = capacity\n'
+ '        self.position = 0\n'
+ '\n'
+ '    def push(self, state, next_state, action, reward):\n'
+ '\n'
+ '        if len(self.memory) < self.capacity:\n'
+ '            self.memory.append(Transition(state, next_state, action, reward))\n'
+ '        self.memory[self.position] = Transition(state, next_state, action, reward)\n'
+ '        self.position = (self.position + 1) % self.capacity\n'
+ '\n'
+ '    def sample(self, batch_size):\n'
+ '        transitions = random.sample(self.memory, batch_size)\n'
+ '        batch = Transition(*zip(*transitions))\n'
+ '        return batch\n'
+ '\n'
+ '    def __len__(self):\n'
+ '        return len(self.memory)\n';

var HELPER_DQN = 
'#!/usr/bin/python3\n'
+ '\n'
+ '# Author(s): Luiz Felipe Vecchietti, Kyujin Choi, Taeyoung Kim\n'
+ '# Maintainer: Kyujin Choi (nav3549@kaist.ac.kr)\n'
+ '\n'
+ 'import os\n'
+ 'import sys\n'
+ 'import math\n'
+ '\n'
+ 'sys.path.append(os.path.dirname(os.path.realpath(__file__)) + \'/../common\')\n'
+ 'try:\n'
+ '    from participant import Participant, Game, Frame\n'
+ 'except ImportError as err:\n'
+ '    print(\'player_random-walk: "participant" module cannot be imported:\', err)\n'
+ '    raise\n'
+ '\n'
+ '#reset_reason\n'
+ 'NONE = Game.NONE\n'
+ 'GAME_START = Game.GAME_START\n'
+ 'SCORE_MYTEAM = Game.SCORE_MYTEAM\n'
+ 'SCORE_OPPONENT = Game.SCORE_OPPONENT\n'
+ 'GAME_END = Game.GAME_END\n'
+ 'DEADLOCK = Game.DEADLOCK\n'
+ 'GOALKICK = Game.GOALKICK\n'
+ 'CORNERKICK = Game.CORNERKICK\n'
+ 'PENALTYKICK = Game.PENALTYKICK\n'
+ 'HALFTIME = Game.HALFTIME\n'
+ 'EPISODE_END = Game.EPISODE_END\n'
+ '\n'
+ '#game_state\n'
+ 'STATE_DEFAULT = Game.STATE_DEFAULT\n'
+ 'STATE_KICKOFF = Game.STATE_KICKOFF\n'
+ 'STATE_GOALKICK = Game.GOALKICK\n'
+ 'STATE_CORNERKICK = Game.CORNERKICK\n'
+ 'STATE_PENALTYKICK = Game.STATE_PENALTYKICK\n'
+ '\n'
+ '#coordinates\n'
+ 'MY_TEAM = Frame.MY_TEAM\n'
+ 'OP_TEAM = Frame.OP_TEAM\n'
+ 'BALL = Frame.BALL\n'
+ 'X = Frame.X\n'
+ 'Y = Frame.Y\n'
+ 'Z = Frame.Z\n'
+ 'TH = Frame.TH\n'
+ 'ACTIVE = Frame.ACTIVE\n'
+ 'TOUCH = Frame.TOUCH\n'
+ 'BALL_POSSESSION = Frame.BALL_POSSESSION\n'
+ '\n'
+ '#robot_index\n'
+ 'GK_INDEX = 0\n'
+ 'D1_INDEX = 1\n'
+ 'D2_INDEX = 2\n'
+ 'F1_INDEX = 3\n'
+ 'F2_INDEX = 4\n'
+ '\n'
+ 'class Discrete():\n'
+ '    # 7 actions: go forwards, go forwards+kick, go forwards + jump,\n'
+ '    #            go backwards, rotate right, rotate left, stop\n'
+ '    WHEELS = [[1,  1,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [1,  1,  5.0,  0.0,  0.0, 0.0],\n'
+ '            [1,  1,  10.0,  10.0,  0.0, 0.0],\n'
+ '            [-1, -1,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [0.0,  0.0,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [0.2, -0.2,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [-0.2,  0.2,  0.0,  0.0,  0.0, 0.0]]\n'
+ '\n'
+ 'class ExtendedDiscrete():\n'
+ '    # 13 actions: go forwards, go forwards+kick, go forwards + jump,\n'
+ '    #            go backwards, rotate right, rotate left, stop\n'
+ '    WHEELS = [[1,  1,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [1,  1,  5.0,  0.0,  0.0, 0.0],\n'
+ '            [1,  1,  10.0,  10.0,  0.0, 0.0],\n'
+ '            [-1, -1,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [0.5, -0.5,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [-0.5, 0.5,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [0.0,  0.0,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [0.0,  0.0,  10.0,  0.0,  0.0, 0.0],\n'
+ '            [0.0,  0.0,  10.0,  8.0,  0.0, 0.0],\n'
+ '            [0.5, 0.5,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [-0.5, -0.5,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [0.2, -0.2,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [-0.2,  0.2,  0.0,  0.0,  0.0, 0.0]]\n'
+ '\n'
+ 'def ball_is_checkpoint(ball, field):\n'
+ '    number_of_checkpoints = 8\n'
+ '    checkpoint = 0\n'
+ '    # checkpoint 1\n'
+ '    if (-field[X]/2 <= ball[X] <= -field[X]*5/8):\n'
+ '        checkpoint = 1\n'
+ '    # checkpoint 2\n'
+ '    if (-field[X]*5/8 < ball[X] <= -field[X]/4):\n'
+ '        checkpoint = 2\n'
+ '    # checkpoint 3\n'
+ '    if (-field[X]/4 < ball[X] <= -field[X]/8):\n'
+ '        checkpoint = 3\n'
+ '    # checkpoint 4\n'
+ '    if (-field[X]/8 < ball[X] <= 0):\n'
+ '        checkpoint = 4\n'
+ '    # checkpoint 5\n'
+ '    if (0 < ball[X] <= field[X]/8):\n'
+ '        checkpoint = 5\n'
+ '    # checkpoint 6\n'
+ '    if (field[X]/8 < ball[X] <= field[X]/4):\n'
+ '        checkpoint = 6\n'
+ '    # checkpoint 7\n'
+ '    if (field[X]/4 < ball[X] <= field[X]*5/8):\n'
+ '        checkpoint = 7\n'
+ '    # checkpoint 8\n'
+ '    if (field[X]*5/8 < ball[X] <= field[X]/2):\n'
+ '        checkpoint = 8\n'
+ '\n'
+ '    return checkpoint\n'
+ '\n'
+ 'def get_velocity(cur_pos, prev_pos, ts):\n'
+ '    vx = (cur_pos[X] - prev_pos[X])/ts\n'
+ '    vy = (cur_pos[Y] - prev_pos[Y])/ts\n'
+ '    vd = math.atan2(vy, vx)\n'
+ '    vr = math.sqrt(math.pow(vx, 2) + math.pow(vy, 2))\n'
+ '    return [vd*180/math.pi, vr]\n'
+ '\n'
+ 'def distance(x1, x2, y1, y2):\n'
+ '    return math.sqrt(math.pow(x1 - x2, 2) + math.pow(y1 - y2, 2))\n'
+ '\n'
+ 'def printConsole(message):\n'
+ '    print(message)\n'
+ '    sys.__stdout__.flush()\n'
+ '\n'
+ 'def predict_ball(cur_ball, previous_ball):\n'
+ '    prediction_step = 2\n'
+ '    dx = cur_ball[X] - previous_ball[X]\n'
+ '    dy = cur_ball[Y] - previous_ball[Y]\n'
+ '    predicted_ball = [cur_ball[X] + prediction_step*dx, cur_ball[Y] + prediction_step*dy]\n'
+ '    return predicted_ball\n'
+ '\n'
+ 'def get_defense_kick_angle(predicted_ball, field, cur_ball):\n'
+ '    if predicted_ball[X] >= -field[X] / 2:\n'
+ '        x = -field[X] / 2 - predicted_ball[X]\n'
+ '    else:\n'
+ '        x = -field[X] / 2 - cur_ball[X]\n'
+ '    y = predicted_ball[Y]\n'
+ '    return math.atan2(y, abs(x) + 0.00001)\n'
+ '\n'
+ 'def degree2radian(deg):\n'
+ '    return deg * math.pi / 180\n'
+ '\n'
+ 'def wrap_to_pi(theta):\n'
+ '    while (theta > math.pi):\n'
+ '        theta -= 2 * math.pi\n'
+ '    while (theta < -math.pi):\n'
+ '        theta += 2 * math.pi\n'
+ '    return theta\n'
+ '\n'
+ 'def go_to(id, x, y, cur_posture, cur_ball, max_linear_velocity):\n'
+ '    sign = 1\n'
+ '    kd = 5\n'
+ '    ka = 0.25\n'
+ '\n'
+ '    tod = 0.005 # tolerance of distance\n'
+ '    tot = math.pi/360 # tolerance of theta\n'
+ '\n'
+ '    dx = x - cur_posture[id][X]\n'
+ '    dy = y - cur_posture[id][Y]\n'
+ '    d_e = math.sqrt(math.pow(dx, 2) + math.pow(dy, 2))\n'
+ '    desired_th = math.atan2(dy, dx)\n'
+ '\n'
+ '    d_th = wrap_to_pi(desired_th - cur_posture[id][TH])\n'
+ '\n'
+ '    if (d_th > degree2radian(90)):\n'
+ '        d_th -= math.pi\n'
+ '        sign = -1\n'
+ '    elif (d_th < degree2radian(-90)):\n'
+ '        d_th += math.pi\n'
+ '        sign = -1\n'
+ '\n'
+ '    if (d_e < tod):\n'
+ '        kd = 0\n'
+ '    if (abs(d_th) < tot):\n'
+ '        ka = 0\n'
+ '\n'
+ '    left_wheel, right_wheel = set_wheel_velocity(max_linear_velocity,\n'
+ '                  sign * (kd * d_e - ka * d_th), \n'
+ '                  sign * (kd * d_e + ka * d_th))\n'
+ '\n'
+ '    return left_wheel, right_wheel\n'
+ '\n'
+ 'def set_wheel_velocity(max_linear_velocity, left_wheel, right_wheel):\n'
+ '    ratio_l = 1\n'
+ '    ratio_r = 1\n'
+ '\n'
+ '    if (left_wheel > max_linear_velocity or right_wheel > max_linear_velocity):\n'
+ '        diff = max(left_wheel, right_wheel) - max_linear_velocity\n'
+ '        left_wheel -= diff\n'
+ '        right_wheel -= diff\n'
+ '    if (left_wheel < -max_linear_velocity or right_wheel < -max_linear_velocity):\n'
+ '        diff = min(left_wheel, right_wheel) + max_linear_velocity\n'
+ '        left_wheel -= diff\n'
+ '        right_wheel -= diff\n'
+ '\n'
+ '    return left_wheel, right_wheel\n';

var NETWORKS_DQN = 
'#!/usr/bin/python3\n'
+ '\n'
+ '# Author(s): Luiz Felipe Vecchietti, Kyujin Choi, Taeyoung Kim\n'
+ '# Maintainer: Kyujin Choi (nav3549@kaist.ac.kr)\n'
+ '\n'
+ 'import os\n'
+ 'import json\n'
+ '\n'
+ 'import torch\n'
+ 'import torch.nn as nn\n'
+ 'import torch.nn.functional as F\n'
+ '\n'
+ 'class Agent(nn.Module):\n'
+ '    def __init__(self, num_inputs, num_outputs):\n'
+ '        super(Agent, self).__init__()\n'
+ '        params_file = open(os.path.dirname(__file__) + \'/parameters.json\')\n'
+ '        params = json.loads(params_file.read())\n'
+ '        self.hidden_dim = params[\'agent_parameters\'][\'hidden_dim\']\n'
+ '        self.input_linear = torch.nn.Linear(num_inputs, self.hidden_dim)\n'
+ '        self.middle_linear = torch.nn.Linear(self.hidden_dim, self.hidden_dim)\n'
+ '        self.output_linear = torch.nn.Linear(self.hidden_dim, num_outputs)\n'
+ '        self.num_layers = 3\n'
+ '\n'
+ '    def forward(self, x):\n'
+ '\n'
+ '        h_relu = self.input_linear(x).clamp(min=0)\n'
+ '        for _ in range(0, self.num_layers):\n'
+ '            h_relu = self.middle_linear(h_relu).clamp(min=0)\n'
+ '        q_pred = self.output_linear(h_relu)\n'
+ '        return q_pred\n'
+ '\n'
+ '    def save_model(self, net, filename):\n'
+ '        torch.save(net.state_dict(), filename)\n';

var RL_UTILS_DQN = 
'#!/usr/bin/python3\n'
+ '\n'
+ '# Author(s): Luiz Felipe Vecchietti, Kyujin Choi, Taeyoung Kim\n'
+ '# Maintainer: Kyujin Choi (nav3549@kaist.ac.kr)\n'
+ '\n'
+ 'import os\n'
+ 'import sys\n'
+ '\n'
+ 'sys.path.append(os.path.dirname(os.path.realpath(__file__)) + \'/../common\')\n'
+ 'try:\n'
+ '    from participant import Participant, Game, Frame\n'
+ 'except ImportError as err:\n'
+ '    print(\'player_random-walk: "participant" module cannot be imported:\', err)\n'
+ '    raise\n'
+ '\n'
+ 'try:\n'
+ '    import _pickle as pickle\n'
+ 'except:\n'
+ '    import pickle\n'
+ 'import math\n'
+ 'import numpy as np\n'
+ 'import torch\n'
+ 'import matplotlib.pyplot as plt\n'
+ 'from torch.autograd import Variable\n'
+ '\n'
+ 'import helper\n'
+ 'from helper import Discrete, ExtendedDiscrete\n'
+ '\n'
+ '#reset_reason\n'
+ 'NONE = Game.NONE\n'
+ 'GAME_START = Game.GAME_START\n'
+ 'SCORE_MYTEAM = Game.SCORE_MYTEAM\n'
+ 'SCORE_OPPONENT = Game.SCORE_OPPONENT\n'
+ 'GAME_END = Game.GAME_END\n'
+ 'DEADLOCK = Game.DEADLOCK\n'
+ 'GOALKICK = Game.GOALKICK\n'
+ 'CORNERKICK = Game.CORNERKICK\n'
+ 'PENALTYKICK = Game.PENALTYKICK\n'
+ 'HALFTIME = Game.HALFTIME\n'
+ 'EPISODE_END = Game.EPISODE_END\n'
+ '\n'
+ '#game_state\n'
+ 'STATE_DEFAULT = Game.STATE_DEFAULT\n'
+ 'STATE_KICKOFF = Game.STATE_KICKOFF\n'
+ 'STATE_GOALKICK = Game.GOALKICK\n'
+ 'STATE_CORNERKICK = Game.CORNERKICK\n'
+ 'STATE_PENALTYKICK = Game.STATE_PENALTYKICK\n'
+ '\n'
+ '#coordinates\n'
+ 'MY_TEAM = Frame.MY_TEAM\n'
+ 'OP_TEAM = Frame.OP_TEAM\n'
+ 'BALL = Frame.BALL\n'
+ 'X = Frame.X\n'
+ 'Y = Frame.Y\n'
+ 'Z = Frame.Z\n'
+ 'TH = Frame.TH\n'
+ 'ACTIVE = Frame.ACTIVE\n'
+ 'TOUCH = Frame.TOUCH\n'
+ 'BALL_POSSESSION = Frame.BALL_POSSESSION\n'
+ '\n'
+ '#robot_index\n'
+ 'GK_INDEX = 0\n'
+ 'D1_INDEX = 1\n'
+ 'D2_INDEX = 2\n'
+ 'F1_INDEX = 3\n'
+ 'F2_INDEX = 4\n'
+ '\n'
+ 'USE_CUDA = torch.cuda.is_available()\n'
+ 'FLOAT = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n'
+ 'LONG = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n'
+ '\n'
+ 'def predict_ball_velocity(cur_ball, prev_ball, ts):\n'
+ '    vx = (cur_ball[X] - prev_ball[X])/ts\n'
+ '    vy = (cur_ball[Y] - prev_ball[Y])/ts\n'
+ '    vd = math.atan2(vy, vx)\n'
+ '    vr = math.sqrt(math.pow(vx, 2) + math.pow(vy, 2))\n'
+ '    return [vd*180/math.pi, vr]\n'
+ '\n'
+ 'def predict_robot_velocity(cur_posture, prev_posture, index, ts):\n'
+ '    vx = (cur_posture[index][X] - prev_posture[index][X])/ts\n'
+ '    vy = (cur_posture[index][Y] - prev_posture[index][Y])/ts\n'
+ '    vd = math.atan2(vy, vx)\n'
+ '    vr = math.sqrt(math.pow(vx, 2) + math.pow(vy, 2))\n'
+ '    return [vd*180/math.pi, vr]\n'
+ '\n'
+ 'def get_state(state_type, cur_posture, prev_posture, cur_posture_opp, prev_posture_opp, cur_ball, prev_ball, field, goal, max_linear_velocity):\n'
+ '\n'
+ '    if state_type == "relative":\n'
+ '        states = get_relative_state(cur_posture, prev_posture, cur_posture_opp, prev_posture_opp, cur_ball, prev_ball, field, goal, max_linear_velocity)\n'
+ '    elif state_type == "full":\n'
+ '        states = get_full_state(cur_posture, prev_posture, cur_posture_opp, prev_posture_opp, cur_ball, prev_ball, field, goal, max_linear_velocity)\n'
+ '    elif state_type == "smm":\n'
+ '        states = get_smm_state(cur_posture, prev_posture, cur_posture_opp, prev_posture_opp, cur_ball, prev_ball, field, goal, max_linear_velocity)\n'
+ '\n'
+ '    return states\n'
+ '\n'
+ 'def get_relative_state(cur_posture, prev_posture, cur_posture_opp, prev_posture_opp, cur_ball, prev_ball, field, goal, max_linear_velocity):\n'
+ '    # relative state: (shape: 8)\n'
+ '    states = [[] for _ in range(5)]\n'
+ '    pxx = field[X] + goal[X]\n'
+ '    pyy = field[Y]\n'
+ '    for i in range(5):\n'
+ '        states[i] =[round((cur_ball[X] - cur_posture[i][X])/pxx, 2), round((cur_ball[Y] - cur_posture[i][Y])/pyy, 2),  \n'
+ '                    round((cur_ball[X] - field[X]/2)/pxx, 2), round((cur_ball[Y] - 0)/pyy, 2),\n'
+ '                    round((-field[X]/2 - cur_posture[i][X])/pxx, 2), round((0 - cur_posture[i][Y])/pyy, 2),\n'
+ '                    round(cur_posture[i][TH], 2), round((math.atan2(cur_ball[Y]-cur_posture[i][Y], cur_ball[X]-cur_posture[i][X]) - cur_posture[i][TH])/math.pi, 2)]\n'
+ '\n'
+ '    return states\n'
+ '\n'
+ 'def get_full_state(cur_posture, prev_posture, cur_posture_opp, prev_posture_opp, cur_ball, prev_ball, field, goal, max_linear_velocity):\n'
+ '    # State information\n'
+ '    # Robot i: x position, y position, orientation, touch information\n'
+ '    # Relative position between ball and robot i\n'
+ '    # Relative velocity between ball and robot i\n'
+ '    # Relative position to other robots in the same team\n'
+ '    # Relative position to other robots in the opponent team\n'
+ '    # Robot velocities from all the robots in the same team\n'
+ '    # Robot velocities from all the robots in the opponent team\n'
+ '    states = [[] for _ in range(5)]\n'
+ '    ball_velocity = predict_ball_velocity(cur_ball, prev_ball, 0.05)\n'
+ '    robot_velocity = [predict_robot_velocity(cur_posture, prev_posture, a, 0.05) for a in range(5)]\n'
+ '    opp_robot_velocity = [predict_robot_velocity(cur_posture_opp, prev_posture_opp, a, 0.05) for a in range(5)]\n'
+ '    px = field[X]/2 + goal[X]\n'
+ '    py = field[Y]/2\n'
+ '    pxx = field[X] + goal[X]\n'
+ '    pyy = field[Y]\n'
+ '    for i in range(5):\n'
+ '        states[i] = [round(cur_posture[i][X]/px, 2), round(cur_posture[i][Y]/py, 2), round(cur_posture[i][TH]/math.pi, 2), round(cur_posture[i][TOUCH], 2),\n'
+ '         round((cur_ball[X] - cur_posture[i][X] )/pxx, 2), round((cur_ball[Y] - cur_posture[i][Y] )/pyy, 2),  round(ball_velocity[1] - robot_velocity[i][1], 2),\n'
+ '         round((cur_posture[GK_INDEX][X] - cur_posture[i][X])/pxx, 2), round((cur_posture[GK_INDEX][Y] - cur_posture[i][Y])/pyy, 2),\n'
+ '         round((cur_posture[D1_INDEX][X] - cur_posture[i][X])/pxx, 2), round((cur_posture[D1_INDEX][Y] - cur_posture[i][Y])/pyy, 2), \n'
+ '         round((cur_posture[D2_INDEX][X] - cur_posture[i][X])/pxx, 2), round((cur_posture[D2_INDEX][Y] - cur_posture[i][Y])/pyy, 2), \n'
+ '         round((cur_posture[F1_INDEX][X] - cur_posture[i][X])/pxx, 2), round((cur_posture[F1_INDEX][Y] - cur_posture[i][Y])/pyy, 2), \n'
+ '         round((cur_posture[F2_INDEX][X] - cur_posture[i][X])/pxx, 2), round((cur_posture[F2_INDEX][Y] - cur_posture[i][Y])/pyy, 2), \n'
+ '         round((cur_posture_opp[GK_INDEX][X] - cur_posture[i][X])/pxx, 2), round((cur_posture_opp[GK_INDEX][Y] - cur_posture[i][Y])/pyy, 2), \n'
+ '         round((cur_posture_opp[D1_INDEX][X] - cur_posture[i][X])/pxx, 2), round((cur_posture_opp[D1_INDEX][Y] - cur_posture[i][Y])/pyy, 2), \n'
+ '         round((cur_posture_opp[D2_INDEX][X] - cur_posture[i][X])/pxx, 2), round((cur_posture_opp[D2_INDEX][Y] - cur_posture[i][Y])/pyy, 2), \n'
+ '         round((cur_posture_opp[F1_INDEX][X] - cur_posture[i][X])/pxx, 2), round((cur_posture_opp[F1_INDEX][Y] - cur_posture[i][Y])/pyy, 2), \n'
+ '         round((cur_posture_opp[F2_INDEX][X] - cur_posture[i][X])/pxx, 2), round((cur_posture_opp[F2_INDEX][Y] - cur_posture[i][Y])/pyy, 2),\n'
+ '         round(max(-1, min(robot_velocity[GK_INDEX][1] / max_linear_velocity[GK_INDEX], 1)), 2), \n'
+ '         round(max(-1, min(robot_velocity[D1_INDEX][1] / max_linear_velocity[D1_INDEX], 1)), 2), \n'
+ '         round(max(-1, min(robot_velocity[D2_INDEX][1] / max_linear_velocity[D2_INDEX], 1)), 2), \n'
+ '         round(max(-1, min(robot_velocity[F1_INDEX][1] / max_linear_velocity[F1_INDEX], 1)), 2), \n'
+ '         round(max(-1, min(robot_velocity[F2_INDEX][1] / max_linear_velocity[F2_INDEX], 1)), 2),\n'
+ '         round(max(-1, min(opp_robot_velocity[GK_INDEX][1] / max_linear_velocity[GK_INDEX], 1)), 2), \n'
+ '         round(max(-1, min(opp_robot_velocity[D1_INDEX][1] / max_linear_velocity[D1_INDEX], 1)), 2), \n'
+ '         round(max(-1, min(opp_robot_velocity[D2_INDEX][1] / max_linear_velocity[D2_INDEX], 1)), 2), \n'
+ '         round(max(-1, min(opp_robot_velocity[F1_INDEX][1] / max_linear_velocity[F1_INDEX], 1)), 2), \n'
+ '         round(max(-1, min(opp_robot_velocity[F2_INDEX][1] / max_linear_velocity[F2_INDEX], 1)), 2)]\n'
+ '    return states\n'
+ '\n'
+ 'def get_smm_state(cur_posture, prev_posture, cur_posture_opp, prev_posture_opp, cur_ball, prev_ball, field, goal, max_linear_velocity):\n'
+ '    # return the minimap given the raw position and orientations of the players\n'
+ '    # returns: (shape: (N, H, W, C)) \n'
+ '    # N: number of players to get_smm_state 5\n'
+ '    # H: height 47\n'
+ '    # W: width 87\n'
+ '    # C: channels 5\n'
+ '    SMM_WIDTH = 87\n'
+ '    SMM_HEIGHT = 47\n'
+ '    _MARKER_VALUE = 1\n'
+ '    # position is binary 0 or 1 \n'
+ '    # orientation is continuous [-1, 1]\n'
+ '    SMM_LAYERS = [\'our_team\', \'opp_team\', \'ball\', \'active_player\', \'active_player_orientation\']\n'
+ '    channel_dimensions = (SMM_WIDTH, SMM_HEIGHT)\n'
+ '    \n'
+ '    # initialize smm states\n'
+ '    states = np.zeros((len(cur_posture), channel_dimensions[1], channel_dimensions[0], len(SMM_LAYERS)))\n'
+ '    # first layer: our team\n'
+ '    for i in range(5):\n'
+ '        x = int((cur_posture[i][X] + field[X]/2 + goal[X])*10)\n'
+ '        y = int((cur_posture[i][Y] + field[Y]/2)*10)\n'
+ '        x = max(0, min(SMM_WIDTH - 1, x))\n'
+ '        y = max(0, min(SMM_HEIGHT - 1, y))\n'
+ '        states[i][y][x][3] = _MARKER_VALUE\n'
+ '        states[i][y][x][4] = cur_posture[i][TH]/math.pi\n'
+ '        for j in range(5):\n'
+ '            states[j][y][x][0] = _MARKER_VALUE\n'
+ '    # second layer: opponent team\n'
+ '    for i in range(5):\n'
+ '        x = int((cur_posture_opp[i][X] + field[X]/2 + goal[X])*10)\n'
+ '        y = int((cur_posture_opp[i][Y] + field[Y]/2)*10)\n'
+ '        x = max(0, min(SMM_WIDTH - 1, x))\n'
+ '        y = max(0, min(SMM_HEIGHT - 1, y))\n'
+ '        for j in range(5):\n'
+ '            states[j][y][x][1] = _MARKER_VALUE\n'
+ '    # third layer: ball\n'
+ '    x = int((cur_ball[X] + field[X]/2 + goal[X])*10)\n'
+ '    y = int((cur_ball[Y] + field[Y]/2)*10)\n'
+ '    x = max(0, min(SMM_WIDTH - 1, x))\n'
+ '    y = max(0, min(SMM_HEIGHT - 1, y))\n'
+ '    for j in range(5):\n'
+ '        states[j][y][x][2] = _MARKER_VALUE\n'
+ '\n'
+ '    return states\n'
+ '\n'
+ 'def get_reward(reward_type, cur_posture, prev_posture, cur_ball, prev_ball, field, id):\n'
+ '    \n'
+ '    if reward_type == "positive_followball":\n'
+ '        reward = get_positive_reward(cur_posture, prev_posture, cur_ball, prev_ball, field, id)\n'
+ '    if reward_type == "negative_followball":\n'
+ '        reward = get_negative_reward(cur_posture, prev_posture, cur_ball, prev_ball, field, id)\n'
+ '    if reward_type == "basic":\n'
+ '        reward = get_basic_reward(cur_posture, prev_posture, cur_ball, prev_ball, field, id)\n'
+ '    if reward_type == "differential":\n'
+ '        reward = get_differential_reward(cur_posture, prev_posture, cur_ball, prev_ball, field, id)\n'
+ '    if reward_type == "checkpoint":\n'
+ '        reward = get_checkpoint_reward(cur_posture, prev_posture, cur_ball, prev_ball, field, id)\n'
+ '\n'
+ '    return reward\n'
+ '\n'
+ 'def get_positive_reward(cur_posture, prev_posture, cur_ball, prev_ball, field, id):\n'
+ '    dist_robot2ball = helper.distance(cur_posture[id][X] , cur_ball[X], cur_posture[id][Y], cur_ball[Y])\n'
+ '    robot_th_error = abs(math.atan2(cur_ball[Y]-cur_posture[id][Y], cur_ball[X]-cur_posture[id][X]) - cur_posture[id][TH])\n'
+ '    ball_robot_dis = helper.distance(cur_posture[id][X] , cur_ball[X], cur_posture[id][Y], cur_ball[Y])\n'
+ '    ball_robot_dis_prev = helper.distance(prev_posture[id][X] , prev_ball[X], prev_posture[id][Y], prev_ball[Y])\n'
+ '    ball_robot_velocity = (ball_robot_dis_prev - ball_robot_dis)/0.05\n'
+ '\n'
+ '    return (0.5*(math.exp(-1*dist_robot2ball)) + 0.5*(math.exp(-1*robot_th_error)) + np.clip(1-math.exp(-1*(ball_robot_velocity)),0,1))\n'
+ '\n'
+ 'def get_negative_reward(cur_posture, prev_posture, cur_ball, prev_ball, field, id):\n'
+ '    dist_robot2ball = helper.distance(cur_posture[id][X] , cur_ball[X], cur_posture[id][Y], cur_ball[Y])\n'
+ '\n'
+ '    return -(1-math.exp(-1*dist_robot2ball))\n'
+ '\n'
+ 'def get_basic_reward(cur_posture, prev_posture, cur_ball, prev_ball, field, id):\n'
+ '    dist_robot2ball = helper.distance(cur_posture[id][X] , cur_ball[X], cur_posture[id][Y], cur_ball[Y])\n'
+ '    dist_ball2goal = helper.distance(field[X]/2 , cur_ball[X], 0, cur_ball[Y])\n'
+ '    score = 0\n'
+ '    concede = 0\n'
+ '    # need to get the score information from simulator variables\n'
+ '    if cur_ball[X] > field[X]/2 :\n'
+ '        score = 10\n'
+ '    if cur_ball[X] < -field[X]/2 :\n'
+ '        concede = -10  \n'
+ '\n'
+ '    return score + concede - (1-math.exp(-1*dist_robot2ball)) - (1-math.exp(-1*dist_ball2goal))\n'
+ '\n'
+ 'def get_differential_reward(cur_posture, prev_posture, cur_ball, prev_ball, field, id):\n'
+ '    dist_robot2ball = helper.distance(cur_posture[id][X] , cur_ball[X], cur_posture[id][Y], cur_ball[Y])\n'
+ '    prev_dist_robot2ball = helper.distance(prev_posture[id][X] , prev_ball[X], prev_posture[id][Y], prev_ball[Y])\n'
+ '    dist_ball2goal = helper.distance(field[X]/2 , cur_ball[X], 0, cur_ball[Y])\n'
+ '    prev_dist_ball2goal = helper.distance(field[X]/2 , prev_ball[X], 0, prev_ball[Y])\n'
+ '    delta_robot2ball = dist_robot2ball - prev_dist_robot2ball\n'
+ '    delta_ball2goal = dist_ball2goal - prev_dist_ball2goal\n'
+ '    score = 0\n'
+ '    concede = 0\n'
+ '    # need to get the score information from simulator variables\n'
+ '    if cur_ball[X] > field[X]/2 :\n'
+ '        score = 10\n'
+ '    if cur_ball[X] < -field[X]/2 :\n'
+ '        concede = -10  \n'
+ '\n'
+ '    return score + concede - delta_robot2ball - delta_ball2goal\n'
+ '\n'
+ 'def get_checkpoint_reward(cur_posture, prev_posture, cur_ball, prev_ball, field, id):\n'
+ '    cur_checkpoint = helper.ball_is_checkpoint(cur_ball, field)\n'
+ '    prev_checkpoint = helper.ball_is_checkpoint(prev_ball, field)\n'
+ '    score = 0\n'
+ '    concede = 0\n'
+ '    checkpoint_differential = 0\n'
+ '    if cur_checkpoint > prev_checkpoint:\n'
+ '        checkpoint_differential = 1\n'
+ '    elif cur_checkpoint > prev_checkpoint:\n'
+ '        checkpoint_differential = -1\n'
+ '        \n'
+ '    # need to get the score information from simulator variables\n'
+ '    if cur_ball[X] > field[X]/2 :\n'
+ '        score = 10\n'
+ '    if cur_ball[X] < -field[X]/2 :\n'
+ '        concede = -10 \n'
+ '\n'
+ '    return score + concede + checkpoint_differential\n'
+ '\n'
+ 'def get_action(action_type, robot_id, action_number, max_linear_velocity, cur_posture, cur_ball, prev_ball, field):\n'
+ '\n'
+ '    if action_type == "discrete":\n'
+ '        wheels = get_discrete_action(robot_id, action_number, max_linear_velocity)\n'
+ '    if action_type == "extendeddiscrete":\n'
+ '        wheels = get_extendeddiscrete_action(robot_id, action_number, max_linear_velocity)\n'
+ '    if action_type == "controlled":\n'
+ '        wheels = get_controlled_action(robot_id, action_number, max_linear_velocity, cur_posture, cur_ball, prev_ball, field)\n'
+ '\n'
+ '    return wheels\n'
+ '\n'
+ 'def get_controlled_action(robot_id, action_number, max_linear_velocity, cur_posture, cur_ball, prev_ball, field):\n'
+ '    predicted_ball = helper.predict_ball(cur_ball, prev_ball)\n'
+ '\n'
+ '    defense_angle = helper.get_defense_kick_angle(predicted_ball, field, cur_ball)\n'
+ '    defense_x = math.cos(defense_angle)*0.6 - field[X]/2\n'
+ '    defense_y = math.sin(defense_angle)*0.6\n'
+ '    delta = 1.5\n'
+ '\n'
+ '    GK_WHEELS = [[predicted_ball[X],  predicted_ball[Y],  0.0,  0.0,  0.0, 0.0],\n'
+ '                [defense_x,  defense_y,  5.0,  0.0,  0.0, 0.0],\n'
+ '                [defense_x,  defense_y,  10.0,  10.0,  0.0, 0.0],\n'
+ '                [defense_x, defense_y,  0.0,  0.0,  0.0, 0.0],\n'
+ '                [cur_posture[robot_id][X],  cur_posture[robot_id][Y],  0.0,  0.0,  0.0, 0.0],\n'
+ '                [defense_x, 0.0,  0.0,  0.0,  0.0, 0.0],\n'
+ '                [-field[X]/2,  defense_y,  0.0,  0.0,  0.0, 0.0],\n'
+ '                [delta,  defense_y,  10.0,  10.0,  0.0, 0.0]]\n'
+ '\n'
+ '    D1_WHEELS = [[predicted_ball[X],  predicted_ball[Y],  0.0,  0.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X],  predicted_ball[Y],  5.0,  0.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X],  predicted_ball[Y],  10.0,  10.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X]-delta, predicted_ball[Y],  0.0,  0.0,  0.0, 0.0],\n'
+ '                [cur_posture[robot_id][X],  cur_posture[robot_id][Y],  0.0,  0.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X]+delta, predicted_ball[Y],  0.0,  0.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X],  predicted_ball[Y]-delta,  0.0,  0.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X],  predicted_ball[Y]+delta,  10.0,  10.0,  0.0, 0.0]]\n'
+ '\n'
+ '    D2_WHEELS = [[predicted_ball[X],  predicted_ball[Y],  0.0,  0.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X],  predicted_ball[Y],  5.0,  0.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X],  predicted_ball[Y],  10.0,  10.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X]-delta, predicted_ball[Y],  0.0,  0.0,  0.0, 0.0],\n'
+ '                [cur_posture[robot_id][X],  cur_posture[robot_id][Y],  0.0,  0.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X]+delta, predicted_ball[Y],  0.0,  0.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X],  predicted_ball[Y]-delta,  0.0,  0.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X],  predicted_ball[Y]+delta,  10.0,  10.0,  0.0, 0.0]]\n'
+ '\n'
+ '    F1_WHEELS = [[predicted_ball[X],  predicted_ball[Y],  0.0,  0.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X],  predicted_ball[Y],  5.0,  0.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X],  predicted_ball[Y],  10.0,  10.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X]-delta, predicted_ball[Y],  0.0,  0.0,  0.0, 0.0],\n'
+ '                [cur_posture[robot_id][X],  cur_posture[robot_id][Y],  0.0,  0.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X]+delta, predicted_ball[Y],  0.0,  0.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X], predicted_ball[Y]-delta,  0.0,  0.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X], predicted_ball[Y]+delta,  10.0,  10.0,  0.0, 0.0]]\n'
+ '\n'
+ '    F2_WHEELS = [[predicted_ball[X], predicted_ball[Y],  0.0,  0.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X], predicted_ball[Y],  5.0,  0.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X], predicted_ball[Y],  10.0,  10.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X]-delta, predicted_ball[Y],  0.0,  0.0,  0.0, 0.0],\n'
+ '                [cur_posture[robot_id][X],  cur_posture[robot_id][Y],  0.0,  0.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X]+delta, predicted_ball[Y],  0.0,  0.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X], predicted_ball[Y]-delta,  0.0,  0.0,  0.0, 0.0],\n'
+ '                [predicted_ball[X], predicted_ball[Y]+delta,  10.0,  10.0,  0.0, 0.0]]\n'
+ '\n'
+ '    wheel = [GK_WHEELS, D1_WHEELS, D2_WHEELS, F1_WHEELS, F2_WHEELS]\n'
+ '\n'
+ '    wheels = [0 for _ in range(30)]\n'
+ '    x, y = helper.go_to(robot_id, wheel[robot_id][action_number][0], wheel[robot_id][action_number][1], cur_posture, cur_ball, max_linear_velocity[robot_id])\n'
+ '\n'
+ '    wheels[6*robot_id + 0] = x\n'
+ '    wheels[6*robot_id + 1] = y\n'
+ '    wheels[6*robot_id + 2] = wheel[robot_id][action_number][2]\n'
+ '    wheels[6*robot_id + 3] = wheel[robot_id][action_number][3]\n'
+ '    wheels[6*robot_id + 4] = wheel[robot_id][action_number][4]\n'
+ '    wheels[6*robot_id + 5] = wheel[robot_id][action_number][5]\n'
+ '\n'
+ '    return wheels\n'
+ '\n'
+ 'def get_discrete_action(robot_id, action_number, max_linear_velocity):\n'
+ '    # 7 actions\n'
+ '    wheels = [0 for _ in range(30)]\n'
+ '\n'
+ '    wheels[6*robot_id + 0] = Discrete.WHEELS[action_number][0]*max_linear_velocity[robot_id]\n'
+ '    wheels[6*robot_id + 1] = Discrete.WHEELS[action_number][1]*max_linear_velocity[robot_id]\n'
+ '    wheels[6*robot_id + 2] = Discrete.WHEELS[action_number][2]\n'
+ '    wheels[6*robot_id + 3] = Discrete.WHEELS[action_number][3]\n'
+ '    wheels[6*robot_id + 4] = Discrete.WHEELS[action_number][4]\n'
+ '    wheels[6*robot_id + 5] = Discrete.WHEELS[action_number][5]\n'
+ '\n'
+ '    return wheels\n'
+ '\n'
+ 'def get_extendeddiscrete_action(robot_id, action_number, max_linear_velocity):\n'
+ '    # 13 actions\n'
+ '    wheels = [0 for _ in range(30)]\n'
+ '\n'
+ '    wheels[6*robot_id + 0] = ExtendedDiscrete.WHEELS[action_number][0]*max_linear_velocity[robot_id]\n'
+ '    wheels[6*robot_id + 1] = ExtendedDiscrete.WHEELS[action_number][1]*max_linear_velocity[robot_id]\n'
+ '    wheels[6*robot_id + 2] = ExtendedDiscrete.WHEELS[action_number][2]\n'
+ '    wheels[6*robot_id + 3] = ExtendedDiscrete.WHEELS[action_number][3]\n'
+ '    wheels[6*robot_id + 4] = ExtendedDiscrete.WHEELS[action_number][4]\n'
+ '    wheels[6*robot_id + 5] = ExtendedDiscrete.WHEELS[action_number][5]\n'
+ '\n'
+ '    return wheels\n'
+ '\n'
+ 'class Logger():\n'
+ '    def __init__(self):\n'
+ '\n'
+ '        self.episode = []\n'
+ '        self.m_episode = []\n'
+ '        self.value = []\n'
+ '        self.mean_value = []\n'
+ '\n'
+ '    def update(self, episode, value, num):\n'
+ '\n'
+ '        self.episode.append(episode)\n'
+ '        self.value.append(value)\n'
+ '        self.num = num\n'
+ '        if len(self.value) >= self.num :\n'
+ '            self.m_episode.append(episode - self.num/2)\n'
+ '            self.mean_value.append(np.mean(self.value[-self.num:]))\n'
+ '\n'
+ '    def plot(self, name):\n'
+ '        plt.title(str(name))\n'
+ '        plt.plot(self.episode, self.value, c = \'lightskyblue\', label=\'total_reward\') \n'
+ '        plt.plot(self.m_episode, self.mean_value, c = \'b\', label=\'Average_Total_Reward\') \n'
+ '        if len(self.episode) <= 10:\n'
+ '            plt.legend(loc=1)\n'
+ '        plt.savefig(\'../../examples/player_deeplearning-single-dqn_py/TOTAL_\'+str(name)+\'.png\')\n';

var TRAIN_DQN = 
'#!/usr/bin/python3\n'
+ '\n'
+ '# Author(s): Luiz Felipe Vecchietti, Kyujin Choi, Taeyoung Kim\n'
+ '# Maintainer: Kyujin Choi (nav3549@kaist.ac.kr)\n'
+ '\n'
+ 'import random\n'
+ 'import os\n'
+ 'import json\n'
+ 'import sys\n'
+ 'sys.path.append(os.path.dirname(os.path.realpath(__file__)) + \'/../common\')\n'
+ 'try:\n'
+ '    from participant import Participant, Game, Frame\n'
+ 'except ImportError as err:\n'
+ '    print(\'player_random-walk: "participant" module cannot be imported:\', err)\n'
+ '    raise\n'
+ '\n'
+ 'import math\n'
+ 'import numpy as np\n'
+ '\n'
+ 'import helper\n'
+ 'from dqn import DQN\n'
+ 'from rl_utils import get_action, get_reward, get_state, Logger\n'
+ '\n'
+ '#reset_reason\n'
+ 'NONE = Game.NONE\n'
+ 'GAME_START = Game.GAME_START\n'
+ 'SCORE_MYTEAM = Game.SCORE_MYTEAM\n'
+ 'SCORE_OPPONENT = Game.SCORE_OPPONENT\n'
+ 'GAME_END = Game.GAME_END\n'
+ 'DEADLOCK = Game.DEADLOCK\n'
+ 'GOALKICK = Game.GOALKICK\n'
+ 'CORNERKICK = Game.CORNERKICK\n'
+ 'PENALTYKICK = Game.PENALTYKICK\n'
+ 'HALFTIME = Game.HALFTIME\n'
+ 'EPISODE_END = Game.EPISODE_END\n'
+ '\n'
+ '#game_state\n'
+ 'STATE_DEFAULT = Game.STATE_DEFAULT\n'
+ 'STATE_KICKOFF = Game.STATE_KICKOFF\n'
+ 'STATE_GOALKICK = Game.GOALKICK\n'
+ 'STATE_CORNERKICK = Game.CORNERKICK\n'
+ 'STATE_PENALTYKICK = Game.STATE_PENALTYKICK\n'
+ '\n'
+ '#coordinates\n'
+ 'MY_TEAM = Frame.MY_TEAM\n'
+ 'OP_TEAM = Frame.OP_TEAM\n'
+ 'BALL = Frame.BALL\n'
+ 'X = Frame.X\n'
+ 'Y = Frame.Y\n'
+ 'Z = Frame.Z\n'
+ 'TH = Frame.TH\n'
+ 'ACTIVE = Frame.ACTIVE\n'
+ 'TOUCH = Frame.TOUCH\n'
+ 'BALL_POSSESSION = Frame.BALL_POSSESSION\n'
+ '\n'
+ '#robot_index\n'
+ 'GK_INDEX = 0 \n'
+ 'D1_INDEX = 1 \n'
+ 'D2_INDEX = 2 \n'
+ 'F1_INDEX = 3 \n'
+ 'F2_INDEX = 4\n'
+ '\n'
+ 'class Frame(object):\n'
+ '    def __init__(self):\n'
+ '        self.time = None\n'
+ '        self.score = None\n'
+ '        self.reset_reason = None\n'
+ '        self.game_state = None\n'
+ '        self.subimages = None\n'
+ '        self.coordinates = None\n'
+ '        self.half_passed = None\n'
+ '\n'
+ 'class Player(Participant):\n'
+ '    def init(self, info):\n'
+ '        params_file = open(os.path.dirname(__file__) + \'/parameters.json\')\n'
+ '        params = json.loads(params_file.read())\n'
+ '        self.field = info[\'field\']\n'
+ '        self.max_linear_velocity = info[\'max_linear_velocity\']\n'
+ '        self.goal = info[\'goal\']\n'
+ '        self.number_of_robots = info[\'number_of_robots\']\n'
+ '        self.end_of_frame = False\n'
+ '        self._frame = 0 \n'
+ '        self.wheels = [0 for _ in range(30)]\n'
+ '        self.cur_posture = []\n'
+ '        self.prev_posture = []\n'
+ '        self.cur_posture_opp = []\n'
+ '        self.cur_ball = []\n'
+ '        self.prev_ball = []\n'
+ '\n'
+ '        self.state_type = params[\'sim_parameters\'][\'state_type\']\n'
+ '        self.action_type = params[\'sim_parameters\'][\'action_type\']\n'
+ '        self.reward_type = params[\'sim_parameters\'][\'reward_type\']\n'
+ '\n'
+ '        self.previous_frame = Frame()\n'
+ '        self.frame_skip = params[\'sim_parameters\'][\'frame_skip\'] # number of frames to skip\n'
+ '        if ( self.state_type == \'relative\'):\n'
+ '            self.obs_size = 8 # state size\n'
+ '        elif ( self.state_type == \'full\'):\n'
+ '            self.obs_size = 37 # state size      \n'
+ '        if (self.action_type == \'discrete\'):\n'
+ '            self.act_size = 7 # number of discrete actions\n'
+ '        elif (self.action_type == \'extendeddiscrete\'):\n'
+ '            self.act_size = 13 # number of discrete actions \n'
+ '        elif (self.action_type == \'continuous\'):\n'
+ '            self.act_size = 2 # number of discrete actions\n'
+ '        elif (self.action_type == \'controlled\'):\n'
+ '            self.act_size = 8 # number of discrete actions\n'
+ '        self.robot_index = np.where(params[\'sim_parameters\'][\'robot\'])[0][0]\n'
+ '\n'
+ '        # for RL\n'
+ '        self.number_of_agents = 1 # in this example, just F2 is trained\n'
+ '        self.action = 0\n'
+ '        self.previous_action = 0\n'
+ '        self.state = []\n'
+ '        self.previous_state = []\n'
+ '        self.reward = 0\n'
+ '        self.previous_reward = 0\n'
+ '\n'
+ '        # RL algorithm class\n'
+ '        if ( params[\'sim_parameters\'][\'algorithm\'] == \'dqn\'):\n'
+ '            self.trainer = DQN(self.number_of_agents, self.obs_size, self.act_size)\n'
+ '        \n'
+ '        # log rewards\n'
+ '        self.total_reward = 0\n'
+ '        self.rew = np.zeros(4)\n'
+ '        # for logging rewards\n'
+ '        self.t = 0\n'
+ '        self.episode = 1\n'
+ '        self.plot_reward = Logger()\n'
+ '        self.save_png_interval = 10\n'
+ '        helper.printConsole("Initializing variables...")\n'
+ '\n'
+ '    def get_coord(self, received_frame):\n'
+ '        self.cur_ball = received_frame.coordinates[BALL]\n'
+ '        self.cur_posture = received_frame.coordinates[MY_TEAM]\n'
+ '        self.cur_posture_opp = received_frame.coordinates[OP_TEAM]\n'
+ '        self.prev_posture = self.previous_frame.coordinates[MY_TEAM]\n'
+ '        self.prev_posture_opp = self.previous_frame.coordinates[OP_TEAM]\n'
+ '        self.prev_ball = self.previous_frame.coordinates[BALL]\n'
+ '\n'
+ '    def update(self, received_frame):\n'
+ '\n'
+ '        if received_frame.end_of_frame:\n'
+ '        \n'
+ '            self._frame += 1\n'
+ '\n'
+ '            if (self._frame == 1):\n'
+ '                self.previous_frame = received_frame\n'
+ '                self.get_coord(received_frame)\n'
+ '\n'
+ '            self.get_coord(received_frame)\n'
+ '\n'
+ '            if self._frame % self.frame_skip == 1:\n'
+ '        \n'
+ '                # Get reward and state\n'
+ '                self.reward = get_reward(self.reward_type, self.cur_posture, self.prev_posture, self.cur_ball, self.prev_ball, self.field, F2_INDEX)\n'
+ '                state = get_state(self.state_type, self.cur_posture, self.prev_posture, self.cur_posture_opp, self.prev_posture_opp, self.cur_ball, self.prev_ball, self.field, self.goal, self.max_linear_velocity) \n'
+ '\n'
+ '                # select next action / only training for F2 robot\n'
+ '                self.state = np.reshape([state[self.robot_index]],(1, self.obs_size))\n'
+ '                self.action = self.trainer.select_action(self.state)\n'
+ '  \n'
+ '                self.total_reward += self.reward\n'
+ '                self.t += 1 \n'
+ '\n'
+ '                # just store experiences if the F2 robot is active\n'
+ '                if self.cur_posture[self.robot_index][ACTIVE]:\n'
+ '                    if self._frame == 1:\n'
+ '                        self.trainer.store_experience(self.state, self.state, self.previous_action, self.previous_reward)\n'
+ '                    else:\n'
+ '                        self.trainer.store_experience(self.previous_state, self.state, self.previous_action, self.previous_reward)\n'
+ '\n'
+ '            else:\n'
+ '                self.action = self.previous_action\n'
+ '\n'
+ '            # Set wheel speeds and send to the simulator\n'
+ '            self.wheels = get_action(self.action_type, self.robot_index, self.action, self.max_linear_velocity, self.cur_posture, self.cur_ball, self.prev_ball, self.field)\n'
+ '            self.set_speeds(self.wheels)\n'
+ '\n'
+ '            # Training script: called every timestep  \n'
+ '            self.trainer.update()\n'
+ '                \n'
+ '            # save checkpoint\n'
+ '            self.trainer.save_checkpoint(self._frame)\n'
+ '            \n'
+ '            # logging training agents reward and plot graph \n'
+ '            if (received_frame.reset_reason > 1) :\n'
+ '\n'
+ '                if self.t >= 10:\n'
+ '                    mean_total_reward = self.total_reward/(self.t)\n'
+ '                    self.plot_reward.update(self.episode, mean_total_reward, 5)\n'
+ '                    if self.episode % self.save_png_interval == 0:\n'
+ '                        self.plot_reward.plot(\'DQN-AGENT-REWARD\') \n'
+ '                    self.episode += 1\n'
+ '                # reset episode timesteps and total reward \n'
+ '                self.t = 0\n'
+ '                self.total_reward = 0\n'
+ '\n'
+ '            # save to update the replay buffer in the next state\n'
+ '            self.previous_state = self.state\n'
+ '            self.previous_action = self.action\n'
+ '            self.previous_reward = self.reward\n'
+ '\n'
+ '            self.end_of_frame = False\n'
+ '            self.previous_frame = received_frame\n'
+ '\n'
+ 'if __name__ == \'__main__\':\n'
+ '    player = Player()\n'
+ '    player.run()\n';

var PLAY_DQN = 
'#!/usr/bin/python3\n'
+ '\n'
+ '# Author(s): Luiz Felipe Vecchietti, Kyujin Choi, Taeyoung Kim\n'
+ '# Maintainer: Kyujin Choi (nav3549@kaist.ac.kr)\n'
+ '\n'
+ 'import random\n'
+ 'import os\n'
+ 'import json\n'
+ 'import sys\n'
+ 'sys.path.append(os.path.dirname(os.path.realpath(__file__)) + \'/../common\')\n'
+ 'try:\n'
+ '    from participant import Participant, Game, Frame\n'
+ 'except ImportError as err:\n'
+ '    print(\'player_random-walk: "participant" module cannot be imported:\', err)\n'
+ '    raise\n'
+ '\n'
+ 'import sys\n'
+ '\n'
+ 'import math\n'
+ 'import numpy as np\n'
+ '\n'
+ 'import helper\n'
+ 'from dqn import DQN\n'
+ 'from rl_utils import  get_action, get_state\n'
+ '\n'
+ '#reset_reason\n'
+ 'NONE = Game.NONE\n'
+ 'GAME_START = Game.GAME_START\n'
+ 'SCORE_MYTEAM = Game.SCORE_MYTEAM\n'
+ 'SCORE_OPPONENT = Game.SCORE_OPPONENT\n'
+ 'GAME_END = Game.GAME_END\n'
+ 'DEADLOCK = Game.DEADLOCK\n'
+ 'GOALKICK = Game.GOALKICK\n'
+ 'CORNERKICK = Game.CORNERKICK\n'
+ 'PENALTYKICK = Game.PENALTYKICK\n'
+ 'HALFTIME = Game.HALFTIME\n'
+ 'EPISODE_END = Game.EPISODE_END\n'
+ '\n'
+ '#game_state\n'
+ 'STATE_DEFAULT = Game.STATE_DEFAULT\n'
+ 'STATE_KICKOFF = Game.STATE_KICKOFF\n'
+ 'STATE_GOALKICK = Game.GOALKICK\n'
+ 'STATE_CORNERKICK = Game.CORNERKICK\n'
+ 'STATE_PENALTYKICK = Game.STATE_PENALTYKICK\n'
+ '\n'
+ '#coordinates\n'
+ 'MY_TEAM = Frame.MY_TEAM\n'
+ 'OP_TEAM = Frame.OP_TEAM\n'
+ 'BALL = Frame.BALL\n'
+ 'X = Frame.X\n'
+ 'Y = Frame.Y\n'
+ 'Z = Frame.Z\n'
+ 'TH = Frame.TH\n'
+ 'ACTIVE = Frame.ACTIVE\n'
+ 'TOUCH = Frame.TOUCH\n'
+ 'BALL_POSSESSION = Frame.BALL_POSSESSION\n'
+ '\n'
+ '#robot_index\n'
+ 'GK_INDEX = 0 \n'
+ 'D1_INDEX = 1 \n'
+ 'D2_INDEX = 2 \n'
+ 'F1_INDEX = 3 \n'
+ 'F2_INDEX = 4\n'
+ '\n'
+ 'class Frame(object):\n'
+ '    def __init__(self):\n'
+ '        self.time = None\n'
+ '        self.score = None\n'
+ '        self.reset_reason = None\n'
+ '        self.game_state = None\n'
+ '        self.subimages = None\n'
+ '        self.coordinates = None\n'
+ '        self.half_passed = None\n'
+ '\n'
+ 'class Player(Participant):\n'
+ '    def init(self, info):\n'
+ '        params_file = open(os.path.dirname(__file__) + \'/parameters.json\')\n'
+ '        params = json.loads(params_file.read())\n'
+ '        self.field = info[\'field\']\n'
+ '        self.max_linear_velocity = info[\'max_linear_velocity\']\n'
+ '        self.goal = info[\'goal\']\n'
+ '        self.number_of_robots = info[\'number_of_robots\']\n'
+ '        self.end_of_frame = False\n'
+ '        self._frame = 0 \n'
+ '        self.wheels = [0 for _ in range(30)]\n'
+ '        self.cur_posture = []\n'
+ '        self.prev_posture = []\n'
+ '        self.cur_posture_opp = []\n'
+ '        self.cur_ball = []\n'
+ '        self.prev_ball = []\n'
+ '\n'
+ '        self.state_type = params[\'sim_parameters\'][\'state_type\']\n'
+ '        self.action_type = params[\'sim_parameters\'][\'action_type\']\n'
+ '\n'
+ '        self.previous_frame = Frame()\n'
+ '        self.frame_skip = params[\'sim_parameters\'][\'frame_skip\'] # number of frames to skip\n'
+ '        if (self.state_type == \'relative\'):\n'
+ '            self.obs_size = 8 # state size\n'
+ '        elif (self.state_type == \'full\'):\n'
+ '            self.obs_size = 37 # state size \n'
+ '        if (self.action_type == \'discrete\'):\n'
+ '            self.act_size = 7 # number of discrete actions\n'
+ '        elif (self.action_type == \'extendeddiscrete\'):\n'
+ '            self.act_size = 13 # number of discrete actions \n'
+ '        elif (self.action_type == \'continuous\'):\n'
+ '            self.act_size = 2 # number of discrete actions\n'
+ '        elif (self.action_type == \'controlled\'):\n'
+ '            self.act_size = 8 # number of discrete actions\n'
+ '        self.robot_index = np.where(params[\'sim_parameters\'][\'robot\'])[0][0]\n'
+ '\n'
+ '        # for RL\n'
+ '        self.action = 0\n'
+ '        self.previous_action = 0\n'
+ '\n'
+ '        self.num_inputs = self.obs_size\n'
+ '        # RL algorithm class\n'
+ '        self.load = True\n'
+ '        self.play = True\n'
+ '        if ( params[\'sim_parameters\'][\'algorithm\'] == \'dqn\'):\n'
+ '            self.trainer = DQN(self.number_of_robots, self.obs_size, self.act_size, self.load, self.play)\n'
+ '\n'
+ '        helper.printConsole("Initializing variables...")\n'
+ '\n'
+ '    def get_coord(self, received_frame):\n'
+ '        self.cur_ball = received_frame.coordinates[BALL]\n'
+ '        self.cur_posture = received_frame.coordinates[MY_TEAM]\n'
+ '        self.cur_posture_opp = received_frame.coordinates[OP_TEAM]\n'
+ '        self.prev_posture = self.previous_frame.coordinates[MY_TEAM]\n'
+ '        self.prev_posture_opp = self.previous_frame.coordinates[OP_TEAM]\n'
+ '        self.prev_ball = self.previous_frame.coordinates[BALL]\n'
+ '\n'
+ '    def update(self, received_frame):\n'
+ '\n'
+ '        if received_frame.end_of_frame:\n'
+ '        \n'
+ '            self._frame += 1\n'
+ '\n'
+ '            if (self._frame == 1):\n'
+ '                self.previous_frame = received_frame\n'
+ '                self.get_coord(received_frame)\n'
+ '\n'
+ '            self.get_coord(received_frame)\n'
+ '\n'
+ '            if self._frame % self.frame_skip == 1:\n'
+ '        \n'
+ '                # Get reward and state\n'
+ '                state = get_state(self.state_type, self.cur_posture, self.prev_posture, self.cur_posture_opp, self.prev_posture_opp, self.cur_ball, self.prev_ball, self.field, self.goal, self.max_linear_velocity) \n'
+ '\n'
+ '                # select next action\n'
+ '                act_input = np.reshape([state[self.robot_index]],(1, self.num_inputs))\n'
+ '                self.action = self.trainer.select_action(act_input)               \n'
+ '\n'
+ '            else:\n'
+ '                self.action = self.previous_action\n'
+ '\n'
+ '            # Set wheel speeds and send to the simulator\n'
+ '            self.wheels = get_action(self.action_type, self.robot_index, self.action, self.max_linear_velocity, self.cur_posture, self.cur_ball, self.prev_ball, self.field)\n'
+ '            self.set_speeds(self.wheels)\n'
+ '\n'
+ '            self.end_of_frame = False\n'
+ '            self.previous_action = self.action\n'
+ '            self.previous_frame = received_frame\n'
+ '\n'
+ 'if __name__ == \'__main__\':\n'
+ '    player = Player()\n'
+ '    player.run()\n';