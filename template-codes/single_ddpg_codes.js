// from: https://github.com/lfelipesv/test_world_3d/blob/blockcode-examples/examples/player_deeplearning-single-ddpg_py

var ALGORITHM_DDPG = 
'import os\n'
+ 'import random\n'
+ 'import json\n'
+ '\n'
+ 'from networks import Actor, Critic\n'
+ 'import torch\n'
+ 'import torch.nn as nn\n'
+ 'import torch.nn.functional as F\n'
+ 'from copy import deepcopy\n'
+ 'import torch.optim as optim\n'
+ 'import numpy as np\n'
+ '\n'
+ 'import helper\n'
+ 'from helper import OrnsteinUhlenbeckProcess\n'
+ 'from episode_memory import Memory\n'
+ 'from rl_utils import to_numpy, to_tensor, soft_update, hard_update\n'
+ '\n'
+ 'CHECKPOINT = os.path.join(os.path.dirname(__file__), \'\')\n'
+ '\n'
+ 'class DDPG(object):\n'
+ '    def __init__(self, nb_states, nb_actions, load=False, play=False):\n'
+ '        params_file = open(os.path.dirname(__file__) + \'/parameters.json\')\n'
+ '        params = json.loads(params_file.read())\n'
+ '        self.nb_states = nb_states\n'
+ '        self.nb_actions= nb_actions\n'
+ '        self.load = load\n'
+ '        self.play = play\n'
+ '\n'
+ '        self._iterations = 0\n'
+ '        self.update_steps = 200\n'
+ '        self.observation_steps = 1000\n'
+ '        self.save_num = 5000\n'
+ '\n'
+ '        # Hyper-parameters\n'
+ '        self.batch_size = params[\'ddpg_parameters\'][\'batch_size\']\n'
+ '        self.tau = 0.001\n'
+ '        self.gamma = params[\'ddpg_parameters\'][\'gamma\']\n'
+ '        self.epsilon = 1.0\n'
+ '        self.depsilon = 1.0/100000\n'
+ '        self.ou_mu = 0.0\n'
+ '        self.ou_sigma = 0.9\n'
+ '        self.ou_theta = 0.9\n'
+ '        actor_lr = params[\'ddpg_parameters\'][\'actor_lr\']\n'
+ '        critic_lr = params[\'ddpg_parameters\'][\'critic_lr\']\n'
+ '        self.grad_norm_clip = params[\'ddpg_parameters\'][\'grad_norm_clip\']\n'
+ '\n'
+ '        self.actor = Actor(self.nb_states, self.nb_actions)\n'
+ '        self.actor_target = Actor(self.nb_states, self.nb_actions)\n'
+ '        self.actor_optim  = optim.Adam(self.actor.parameters(), lr=actor_lr)\n'
+ '\n'
+ '        self.critic = Critic(self.nb_states, self.nb_actions)\n'
+ '        self.critic_target = Critic(self.nb_states, self.nb_actions)\n'
+ '        self.critic_optim  = optim.Adam(self.critic.parameters(), lr=critic_lr)\n'
+ '\n'
+ '        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n'
+ '\n'
+ '        if self.load:\n'
+ '            self.actor.load_state_dict(\n'
+ '                torch.load(os.path.join(CHECKPOINT, "actor.pkl"), map_location=torch.device(self.device)))\n'
+ '            self.critic.load_state_dict(\n'
+ '                torch.load(os.path.join(CHECKPOINT, "critic.pkl"), map_location=torch.device(self.device)))\n'
+ '            helper.printConsole("loading variables...")\n'
+ '\n'
+ '        hard_update(self.actor_target, self.actor) # Make sure target is with the same weight\n'
+ '        hard_update(self.critic_target, self.critic)\n'
+ '        \n'
+ '        if self.play:\n'
+ '            self.eval()\n'
+ '        else:\n'
+ '            self.train()\n'
+ '        self.to_device()\n'
+ '\n'
+ '        #Create replay buffer\n'
+ '        self.memory = Memory(params[\'ddpg_parameters\'][\'buffer_size\'])\n'
+ '        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=self.ou_theta, mu=self.ou_mu, sigma=self.ou_sigma)\n'
+ '\n'
+ '\n'
+ '    def update_policy(self):\n'
+ '        # Sample batch\n'
+ '        batch = self.memory.sample(self.batch_size)\n'
+ '        \n'
+ '        state_batch = torch.Tensor(batch.state).squeeze().to(self.device)\n'
+ '        next_state_batch = torch.Tensor(batch.next_state).squeeze().to(self.device)\n'
+ '        action_batch = torch.Tensor(batch.action).squeeze().to(self.device)\n'
+ '        reward_batch = torch.Tensor(batch.reward).unsqueeze(1).to(self.device)\n'
+ '        terminal_batch = torch.Tensor(batch.terminal).unsqueeze(1).to(self.device)\n'
+ '\n'
+ '        # Prepare for the target q batch\n'
+ '        next_q_values = self.critic_target([\n'
+ '            next_state_batch,\n'
+ '            self.actor_target(next_state_batch),\n'
+ '        ])\n'
+ '\n'
+ '        target_q_batch = reward_batch + \ '
+ '            self.gamma*terminal_batch*next_q_values\n'
+ '\n'
+ '        # Critic update\n'
+ '        self.critic.zero_grad()\n'
+ '\n'
+ '        q_batch = self.critic([state_batch, action_batch])\n'
+ '\n'
+ '        value_loss = nn.MSELoss()(q_batch, target_q_batch)\n'
+ '        value_loss.backward()\n'
+ '        grad_norm_critic = torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.grad_norm_clip)\n'
+ '        self.critic_optim.step()\n'
+ '\n'
+ '        # Actor update\n'
+ '        self.actor.zero_grad()\n'
+ '\n'
+ '        policy_loss = -self.critic([\n'
+ '            state_batch,\n'
+ '            self.actor(state_batch)\n'
+ '        ])\n'
+ '\n'
+ '        policy_loss = policy_loss.mean()\n'
+ '        policy_loss.backward()\n'
+ '        grad_norm_actor = torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.grad_norm_clip)\n'
+ '        self.actor_optim.step()\n'
+ '\n'
+ '        # Target update\n'
+ '        soft_update(self.actor_target, self.actor, self.tau)\n'
+ '        soft_update(self.critic_target, self.critic, self.tau)\n'
+ '        return value_loss.cpu().data.numpy(), policy_loss.cpu().data.numpy()\n'
+ '\n'
+ '    def train(self):\n'
+ '        self.actor.train()\n'
+ '        self.actor_target.train()\n'
+ '        self.critic.train()\n'
+ '        self.critic_target.train()\n'
+ '\n'
+ '    def eval(self):\n'
+ '        self.actor.eval()\n'
+ '        self.actor_target.eval()\n'
+ '        self.critic.eval()\n'
+ '        self.critic_target.eval()\n'
+ '\n'
+ '    def to_device(self):\n'
+ '        self.actor.to(self.device)\n'
+ '        self.actor_target.to(self.device)\n'
+ '        self.critic.to(self.device)\n'
+ '        self.critic_target.to(self.device)\n'
+ '\n'
+ '    def store_experience(self, state, next_state, act, rew, terminal):\n'
+ '        self.memory.push(state, next_state, act, rew, terminal)\n'
+ '\n'
+ '    def select_action(self, s_t):\n'
+ '        action = to_numpy(\n'
+ '            self.actor(to_tensor(np.array([s_t])))\n'
+ '        ).squeeze(0)\n'
+ '        if not self.play:\n'
+ '            action += max(self.epsilon, 0)*self.random_process.sample()\n'
+ '        action = np.clip(action, -1., 1.)\n'
+ '\n'
+ '        if not self.play:\n'
+ '            self.epsilon -= self.depsilon\n'
+ '\n'
+ '        return action\n'
+ '\n'
+ '    def print_loss(self, vloss, ploss, iteration):\n'
+ '        if self._iterations % 100 == 0: # Print information every 100 iterations\n'
+ '            helper.printConsole("======================================================")\n'
+ '            helper.printConsole("Epsilon: " + str(self.epsilon))\n'
+ '            helper.printConsole("iterations: " + str(self._iterations))\n'
+ '            helper.printConsole("Value Loss: " + str(vloss))\n'
+ '            helper.printConsole("Policy Loss: " + str(ploss))\n'
+ '            helper.printConsole("======================================================")\n'
+ '\n'
+ '    def save_checkpoint(self, iteration):\n'
+ '        if iteration % self.save_num == 0:\n'
+ '            torch.save(\n'
+ '                self.actor.state_dict(),\n'
+ '                \'{}/actor.pkl\'.format(CHECKPOINT)\n'
+ '            )\n'
+ '            torch.save(\n'
+ '                self.critic.state_dict(),\n'
+ '                \'{}/critic.pkl\'.format(CHECKPOINT)\n'
+ '            )\n'
+ '            helper.printConsole("Saved Checkpoint.")\n'
+ '\n'
+ '    def update(self):\n'
+ '        if len(self.memory) > self.observation_steps:\n'
+ '            self._iterations += 1\n'
+ '            vloss, ploss = self.update_policy()\n'
+ '            self.print_loss(vloss, ploss, self._iterations) \n';

var EPISODE_MEMORY_DDPG = 
'#!/usr/bin/python3\n'
+ '\n'
+ '# Author(s): Luiz Felipe Vecchietti, Kyujin Choi, Taeyoung Kim\n'
+ '# Maintainer: Kyujin Choi (nav3549@kaist.ac.kr)\n'
+ '\n'
+ 'import random\n'
+ 'import numpy as np\n'
+ 'from collections import namedtuple\n'
+ '\n'
+ 'Transition = namedtuple(\'Transition\', (\'state\', \'next_state\', \'action\', \'reward\', \'terminal\'))\n'
+ '\n'
+ 'class Memory(object):\n'
+ '    def __init__(self, capacity):\n'
+ '        self.memory = []\n'
+ '        self.capacity = capacity\n'
+ '        self.position = 0\n'
+ '\n'
+ '    def push(self, state, next_state, action, reward, terminal):\n'
+ '        if len(self.memory) < self.capacity:\n'
+ '            self.memory.append(Transition(state, next_state, action, reward, terminal))\n'
+ '        self.memory[self.position] = Transition(state, next_state, action, reward, terminal)\n'
+ '        self.position = (self.position + 1) % self.capacity\n'
+ '\n'
+ '    def sample(self, batch_size):\n'
+ '        transitions = random.sample(self.memory, batch_size)\n'
+ '        batch = Transition(*zip(*transitions))\n'
+ '\n'
+ '        return batch\n'
+ '\n'
+ '    def __len__(self):\n'
+ '        return len(self.memory)\n';

var HELPER_DDPG = 
'#!/usr/bin/python3\n'
+ '\n'
+ '# Author(s): Luiz Felipe Vecchietti, Kyujin Choi, Taeyoung Kim\n'
+ '# Maintainer: Kyujin Choi (nav3549@kaist.ac.kr)\n'
+ '\n'
+ 'import sys\n'
+ 'import math\n'
+ 'import numpy as np\n'
+ '\n'
+ '# Based on: https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n'
+ 'class RandomProcess(object):\n'
+ '    def reset_states(self):\n'
+ '        pass\n'
+ '\n'
+ 'class AnnealedGaussianProcess(RandomProcess):\n'
+ '    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n'
+ '        self.mu = mu\n'
+ '        self.sigma = sigma\n'
+ '        self.n_steps = 0\n'
+ '\n'
+ '        if sigma_min is not None:\n'
+ '            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n'
+ '            self.c = sigma\n'
+ '            self.sigma_min = sigma_min\n'
+ '        else:\n'
+ '            self.m = 0.\n'
+ '            self.c = sigma\n'
+ '            self.sigma_min = sigma\n'
+ '\n'
+ '    @property\n'
+ '    def current_sigma(self):\n'
+ '        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n'
+ '        return sigma\n'
+ '\n'
+ '\n'
+ '# Based on: http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n'
+ 'class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n'
+ '    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n'
+ '        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n'
+ '        self.theta = theta\n'
+ '        self.mu = mu\n'
+ '        self.dt = dt\n'
+ '        self.x0 = x0\n'
+ '        self.size = size\n'
+ '        self.reset_states()\n'
+ '\n'
+ '    def sample(self):\n'
+ '        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n'
+ '        self.x_prev = x\n'
+ '        self.n_steps += 1\n'
+ '        return x\n'
+ '\n'
+ '    def reset_states(self):\n'
+ '        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)\n'
+ '\n'
+ 'class Discrete():\n'
+ '    # 7 actions: go forwards, go forwards+kick, go forwards + jump,\n'
+ '    #            go backwards, rotate right, rotate left, stop\n'
+ '    WHEELS = [[1,  1,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [1,  1,  5.0,  0.0,  0.0, 0.0],\n'
+ '            [1,  1,  10.0,  10.0,  0.0, 0.0],\n'
+ '            [-1, -1,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [0.0,  0.0,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [0.2, -0.2,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [-0.2,  0.2,  0.0,  0.0,  0.0, 0.0]]\n'
+ '\n'
+ 'class ExtendedDiscrete():\n'
+ '    # 13 actions: go forwards, go forwards+kick, go forwards + jump,\n'
+ '    #            go backwards, rotate right, rotate left, stop\n'
+ '    WHEELS = [[1,  1,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [1,  1,  5.0,  0.0,  0.0, 0.0],\n'
+ '            [1,  1,  10.0,  10.0,  0.0, 0.0],\n'
+ '            [-1, -1,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [0.5, -0.5,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [-0.5, 0.5,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [0.0,  0.0,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [0.0,  0.0,  10.0,  0.0,  0.0, 0.0],\n'
+ '            [0.0,  0.0,  10.0,  8.0,  0.0, 0.0],\n'
+ '            [0.5, 0.5,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [-0.5, -0.5,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [0.2, -0.2,  0.0,  0.0,  0.0, 0.0],\n'
+ '            [-0.2,  0.2,  0.0,  0.0,  0.0, 0.0]]\n'
+ '\n'
+ 'def ball_is_checkpoint(ball, field):\n'
+ '    number_of_checkpoints = 8\n'
+ '    checkpoint = 0\n'
+ '    # checkpoint 1\n'
+ '    if (-field[X]/2 <= ball[X] <= -field[X]*5/8):\n'
+ '        checkpoint = 1\n'
+ '    # checkpoint 2\n'
+ '    if (-field[X]*5/8 < ball[X] <= -field[X]/4):\n'
+ '        checkpoint = 2\n'
+ '    # checkpoint 3\n'
+ '    if (-field[X]/4 < ball[X] <= -field[X]/8):\n'
+ '        checkpoint = 3\n'
+ '    # checkpoint 4\n'
+ '    if (-field[X]/8 < ball[X] <= 0):\n'
+ '        checkpoint = 4\n'
+ '    # checkpoint 5\n'
+ '    if (0 < ball[X] <= field[X]/8):\n'
+ '        checkpoint = 5\n'
+ '    # checkpoint 6\n'
+ '    if (field[X]/8 < ball[X] <= field[X]/4):\n'
+ '        checkpoint = 6\n'
+ '    # checkpoint 7\n'
+ '    if (field[X]/4 < ball[X] <= field[X]*5/8):\n'
+ '        checkpoint = 7\n'
+ '    # checkpoint 8\n'
+ '    if (field[X]*5/8 < ball[X] <= field[X]/2):\n'
+ '        checkpoint = 8\n'
+ '\n'
+ '    return checkpoint\n'
+ '\n'
+ 'def get_velocity(cur_pos, prev_pos, ts):\n'
+ '    vx = (cur_pos[X] - prev_pos[X])/ts\n'
+ '    vy = (cur_pos[Y] - prev_pos[Y])/ts\n'
+ '    vd = math.atan2(vy, vx)\n'
+ '    vr = math.sqrt(math.pow(vx, 2) + math.pow(vy, 2))\n'
+ '    return [vd*180/math.pi, vr]\n'
+ '\n'
+ 'def distance(x1, x2, y1, y2):\n'
+ '    return math.sqrt(math.pow(x1 - x2, 2) + math.pow(y1 - y2, 2))\n'
+ '\n'
+ 'def printConsole(message):\n'
+ '    print(message)\n'
+ '    sys.__stdout__.flush()\n';

var NETWORKS_DDPG = 
'#!/usr/bin/python3\n'
+ '\n'
+ '# Author(s): Luiz Felipe Vecchietti, Kyujin Choi, Taeyoung Kim\n'
+ '# Maintainer: Kyujin Choi (nav3549@kaist.ac.kr)\n'
+ '\n'
+ 'import torch\n'
+ 'import torch.nn as nn\n'
+ 'import torch.nn.functional as F\n'
+ 'import numpy as np\n'
+ '\n'
+ 'def fanin_init(size, fanin=None):\n'
+ '    fanin = fanin or size[0]\n'
+ '    v = 1. / np.sqrt(fanin)\n'
+ '    return torch.Tensor(size).uniform_(-v, v)\n'
+ '\n'
+ 'class Actor(nn.Module):\n'
+ '    def __init__(self, nb_states, nb_actions, hidden1=400, hidden2=300, init_w=3e-3):\n'
+ '        super(Actor, self).__init__()\n'
+ '        self.fc1 = nn.Linear(nb_states, hidden1)\n'
+ '        self.fc2 = nn.Linear(hidden1, hidden2)\n'
+ '        self.fc3 = nn.Linear(hidden2, nb_actions)\n'
+ '        self.relu = nn.ReLU()\n'
+ '        self.tanh = nn.Tanh()\n'
+ '        self.init_weights(init_w)\n'
+ '    \n'
+ '    def init_weights(self, init_w):\n'
+ '        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n'
+ '        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n'
+ '        self.fc3.weight.data.uniform_(-init_w, init_w)\n'
+ '    \n'
+ '    def forward(self, x):\n'
+ '        out = self.fc1(x)\n'
+ '        out = self.relu(out)\n'
+ '        out = self.fc2(out)\n'
+ '        out = self.relu(out)\n'
+ '        out = self.fc3(out)\n'
+ '        out = self.tanh(out)\n'
+ '        return out\n'
+ '\n'
+ 'class Critic(nn.Module):\n'
+ '    def __init__(self, nb_states, nb_actions, hidden1=400, hidden2=300, init_w=3e-3):\n'
+ '        super(Critic, self).__init__()\n'
+ '        self.fc1 = nn.Linear(nb_states, hidden1)\n'
+ '        self.fc2 = nn.Linear(hidden1+nb_actions, hidden2)\n'
+ '        self.fc3 = nn.Linear(hidden2, 1)\n'
+ '        self.relu = nn.ReLU()\n'
+ '        self.init_weights(init_w)\n'
+ '    \n'
+ '    def init_weights(self, init_w):\n'
+ '        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n'
+ '        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n'
+ '        self.fc3.weight.data.uniform_(-init_w, init_w)\n'
+ '    \n'
+ '    def forward(self, xs):\n'
+ '        x, a = xs\n'
+ '        out = self.fc1(x)\n'
+ '        out = self.relu(out)\n'
+ '        # debug()\n'
+ '        out = self.fc2(torch.cat([out, a],1))\n'
+ '        out = self.relu(out)\n'
+ '        out = self.fc3(out)\n'
+ '        return out\n';

var RL_UTILS_DDPG = 
'#!/usr/bin/python3\n'
+ '\n'
+ '# Author(s): Luiz Felipe Vecchietti, Kyujin Choi, Taeyoung Kim\n'
+ '# Maintainer: Kyujin Choi (nav3549@kaist.ac.kr)\n'
+ '\n'
+ 'import os\n'
+ 'import sys\n'
+ '\n'
+ 'sys.path.append(os.path.dirname(os.path.realpath(__file__)) + \'/../common\')\n'
+ 'try:\n'
+ '    from participant import Participant, Game, Frame\n'
+ 'except ImportError as err:\n'
+ '    print(\'player_random-walk: "participant" module cannot be imported:\', err)\n'
+ '    raise\n'
+ '\n'
+ 'try:\n'
+ '    import _pickle as pickle\n'
+ 'except:\n'
+ '    import pickle\n'
+ 'import math\n'
+ 'import numpy as np\n'
+ 'import torch\n'
+ 'import matplotlib.pyplot as plt\n'
+ 'from torch.autograd import Variable\n'
+ '\n'
+ 'import helper\n'
+ 'from helper import Discrete, ExtendedDiscrete\n'
+ '\n'
+ '#reset_reason\n'
+ 'NONE = Game.NONE\n'
+ 'GAME_START = Game.GAME_START\n'
+ 'SCORE_MYTEAM = Game.SCORE_MYTEAM\n'
+ 'SCORE_OPPONENT = Game.SCORE_OPPONENT\n'
+ 'GAME_END = Game.GAME_END\n'
+ 'DEADLOCK = Game.DEADLOCK\n'
+ 'GOALKICK = Game.GOALKICK\n'
+ 'CORNERKICK = Game.CORNERKICK\n'
+ 'PENALTYKICK = Game.PENALTYKICK\n'
+ 'HALFTIME = Game.HALFTIME\n'
+ 'EPISODE_END = Game.EPISODE_END\n'
+ '\n'
+ '#game_state\n'
+ 'STATE_DEFAULT = Game.STATE_DEFAULT\n'
+ 'STATE_KICKOFF = Game.STATE_KICKOFF\n'
+ 'STATE_GOALKICK = Game.GOALKICK\n'
+ 'STATE_CORNERKICK = Game.CORNERKICK\n'
+ 'STATE_PENALTYKICK = Game.STATE_PENALTYKICK\n'
+ '\n'
+ '#coordinates\n'
+ 'MY_TEAM = Frame.MY_TEAM\n'
+ 'OP_TEAM = Frame.OP_TEAM\n'
+ 'BALL = Frame.BALL\n'
+ 'X = Frame.X\n'
+ 'Y = Frame.Y\n'
+ 'Z = Frame.Z\n'
+ 'TH = Frame.TH\n'
+ 'ACTIVE = Frame.ACTIVE\n'
+ 'TOUCH = Frame.TOUCH\n'
+ 'BALL_POSSESSION = Frame.BALL_POSSESSION\n'
+ '\n'
+ '#robot_index\n'
+ 'GK_INDEX = 0\n'
+ 'D1_INDEX = 1\n'
+ 'D2_INDEX = 2\n'
+ 'F1_INDEX = 3\n'
+ 'F2_INDEX = 4\n'
+ '\n'
+ 'USE_CUDA = torch.cuda.is_available()\n'
+ 'FLOAT = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n'
+ 'LONG = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n'
+ '\n'
+ 'def to_numpy(var):\n'
+ '    return var.cpu().data.numpy() if USE_CUDA else var.data.numpy()\n'
+ '\n'
+ 'def to_tensor(ndarray, volatile=False, requires_grad=False, dtype=FLOAT):\n'
+ '    return Variable(\n'
+ '        torch.from_numpy(ndarray), volatile=volatile, requires_grad=requires_grad\n'
+ '    ).type(dtype)\n'
+ '\n'
+ 'def soft_update(target, source, tau):\n'
+ '    for target_param, param in zip(target.parameters(), source.parameters()):\n'
+ '        target_param.data.copy_(\n'
+ '            target_param.data * (1.0 - tau) + param.data * tau\n'
+ '        )\n'
+ '\n'
+ 'def hard_update(target, source):\n'
+ '    for target_param, param in zip(target.parameters(), source.parameters()):\n'
+ '            target_param.data.copy_(param.data)\n'
+ '\n'
+ 'def predict_ball_velocity(cur_ball, prev_ball, ts):\n'
+ '    vx = (cur_ball[X] - prev_ball[X])/ts\n'
+ '    vy = (cur_ball[Y] - prev_ball[Y])/ts\n'
+ '    vd = math.atan2(vy, vx)\n'
+ '    vr = math.sqrt(math.pow(vx, 2) + math.pow(vy, 2))\n'
+ '    return [vd*180/math.pi, vr]\n'
+ '\n'
+ 'def predict_robot_velocity(cur_posture, prev_posture, index, ts):\n'
+ '    vx = (cur_posture[index][X] - prev_posture[index][X])/ts\n'
+ '    vy = (cur_posture[index][Y] - prev_posture[index][Y])/ts\n'
+ '    vd = math.atan2(vy, vx)\n'
+ '    vr = math.sqrt(math.pow(vx, 2) + math.pow(vy, 2))\n'
+ '    return [vd*180/math.pi, vr]\n'
+ '\n'
+ 'def get_state(state_type, cur_posture, prev_posture, cur_posture_opp, prev_posture_opp, cur_ball, prev_ball, field, goal, max_linear_velocity):\n'
+ '\n'
+ '    if state_type == "relative":\n'
+ '        states = get_relative_state(cur_posture, prev_posture, cur_posture_opp, prev_posture_opp, cur_ball, prev_ball, field, goal, max_linear_velocity)\n'
+ '    elif state_type == "full":\n'
+ '        states = get_full_state(cur_posture, prev_posture, cur_posture_opp, prev_posture_opp, cur_ball, prev_ball, field, goal, max_linear_velocity)\n'
+ '    elif state_type == "smm":\n'
+ '        states = get_smm_state(cur_posture, prev_posture, cur_posture_opp, prev_posture_opp, cur_ball, prev_ball, field, goal, max_linear_velocity)\n'
+ '\n'
+ '    return states\n'
+ '\n'
+ 'def get_relative_state(cur_posture, prev_posture, cur_posture_opp, prev_posture_opp, cur_ball, prev_ball, field, goal, max_linear_velocity):\n'
+ '    # relative state: (shape: 8)\n'
+ '    states = [[] for _ in range(5)]\n'
+ '    pxx = field[X] + goal[X]\n'
+ '    pyy = field[Y]\n'
+ '    for i in range(5):\n'
+ '        states[i] =[round((cur_ball[X] - cur_posture[i][X])/pxx, 2), round((cur_ball[Y] - cur_posture[i][Y])/pyy, 2),  \n'
+ '                    round((cur_ball[X] - field[X]/2)/pxx, 2), round((cur_ball[Y] - 0)/pyy, 2),\n'
+ '                    round((-field[X]/2 - cur_posture[i][X])/pxx, 2), round((0 - cur_posture[i][Y])/pyy, 2),\n'
+ '                    round(cur_posture[i][TH], 2), round((math.atan2(cur_ball[Y]-cur_posture[i][Y], cur_ball[X]-cur_posture[i][X]) - cur_posture[i][TH])/math.pi, 2)]\n'
+ '\n'
+ '    return states\n'
+ '\n'
+ 'def get_full_state(cur_posture, prev_posture, cur_posture_opp, prev_posture_opp, cur_ball, prev_ball, field, goal, max_linear_velocity):\n'
+ '    # State information\n'
+ '    # Robot i: x position, y position, orientation, touch information\n'
+ '    # Relative position between ball and robot i\n'
+ '    # Relative velocity between ball and robot i\n'
+ '    # Relative position to other robots in the same team\n'
+ '    # Relative position to other robots in the opponent team\n'
+ '    # Robot velocities from all the robots in the same team\n'
+ '    # Robot velocities from all the robots in the opponent team\n'
+ '    states = [[] for _ in range(5)]\n'
+ '    ball_velocity = predict_ball_velocity(cur_ball, prev_ball, 0.05)\n'
+ '    robot_velocity = [predict_robot_velocity(cur_posture, prev_posture, a, 0.05) for a in range(5)]\n'
+ '    opp_robot_velocity = [predict_robot_velocity(cur_posture_opp, prev_posture_opp, a, 0.05) for a in range(5)]\n'
+ '    px = field[X]/2 + goal[X]\n'
+ '    py = field[Y]/2\n'
+ '    pxx = field[X] + goal[X]\n'
+ '    pyy = field[Y]\n'
+ '    for i in range(5):\n'
+ '        states[i] = [round(cur_posture[i][X]/px, 2), round(cur_posture[i][Y]/py, 2), round(cur_posture[i][TH]/math.pi, 2), round(cur_posture[i][TOUCH], 2),\n'
+ '         round((cur_ball[X] - cur_posture[i][X] )/pxx, 2), round((cur_ball[Y] - cur_posture[i][Y] )/pyy, 2),  round(ball_velocity[1] - robot_velocity[i][1], 2),\n'
+ '         round((cur_posture[GK_INDEX][X] - cur_posture[i][X])/pxx, 2), round((cur_posture[GK_INDEX][Y] - cur_posture[i][Y])/pyy, 2),\n'
+ '         round((cur_posture[D1_INDEX][X] - cur_posture[i][X])/pxx, 2), round((cur_posture[D1_INDEX][Y] - cur_posture[i][Y])/pyy, 2), \n'
+ '         round((cur_posture[D2_INDEX][X] - cur_posture[i][X])/pxx, 2), round((cur_posture[D2_INDEX][Y] - cur_posture[i][Y])/pyy, 2), \n'
+ '         round((cur_posture[F1_INDEX][X] - cur_posture[i][X])/pxx, 2), round((cur_posture[F1_INDEX][Y] - cur_posture[i][Y])/pyy, 2), \n'
+ '         round((cur_posture[F2_INDEX][X] - cur_posture[i][X])/pxx, 2), round((cur_posture[F2_INDEX][Y] - cur_posture[i][Y])/pyy, 2), \n'
+ '         round((cur_posture_opp[GK_INDEX][X] - cur_posture[i][X])/pxx, 2), round((cur_posture_opp[GK_INDEX][Y] - cur_posture[i][Y])/pyy, 2), \n'
+ '         round((cur_posture_opp[D1_INDEX][X] - cur_posture[i][X])/pxx, 2), round((cur_posture_opp[D1_INDEX][Y] - cur_posture[i][Y])/pyy, 2), \n'
+ '         round((cur_posture_opp[D2_INDEX][X] - cur_posture[i][X])/pxx, 2), round((cur_posture_opp[D2_INDEX][Y] - cur_posture[i][Y])/pyy, 2), \n'
+ '         round((cur_posture_opp[F1_INDEX][X] - cur_posture[i][X])/pxx, 2), round((cur_posture_opp[F1_INDEX][Y] - cur_posture[i][Y])/pyy, 2), \n'
+ '         round((cur_posture_opp[F2_INDEX][X] - cur_posture[i][X])/pxx, 2), round((cur_posture_opp[F2_INDEX][Y] - cur_posture[i][Y])/pyy, 2),\n'
+ '         round(max(-1, min(robot_velocity[GK_INDEX][1] / max_linear_velocity[GK_INDEX], 1)), 2), \n'
+ '         round(max(-1, min(robot_velocity[D1_INDEX][1] / max_linear_velocity[D1_INDEX], 1)), 2), \n'
+ '         round(max(-1, min(robot_velocity[D2_INDEX][1] / max_linear_velocity[D2_INDEX], 1)), 2), \n'
+ '         round(max(-1, min(robot_velocity[F1_INDEX][1] / max_linear_velocity[F1_INDEX], 1)), 2), \n'
+ '         round(max(-1, min(robot_velocity[F2_INDEX][1] / max_linear_velocity[F2_INDEX], 1)), 2),\n'
+ '         round(max(-1, min(opp_robot_velocity[GK_INDEX][1] / max_linear_velocity[GK_INDEX], 1)), 2), \n'
+ '         round(max(-1, min(opp_robot_velocity[D1_INDEX][1] / max_linear_velocity[D1_INDEX], 1)), 2), \n'
+ '         round(max(-1, min(opp_robot_velocity[D2_INDEX][1] / max_linear_velocity[D2_INDEX], 1)), 2), \n'
+ '         round(max(-1, min(opp_robot_velocity[F1_INDEX][1] / max_linear_velocity[F1_INDEX], 1)), 2), \n'
+ '         round(max(-1, min(opp_robot_velocity[F2_INDEX][1] / max_linear_velocity[F2_INDEX], 1)), 2)]\n'
+ '    return states\n'
+ '\n'
+ 'def get_smm_state(cur_posture, prev_posture, cur_posture_opp, prev_posture_opp, cur_ball, prev_ball, field, goal, max_linear_velocity):\n'
+ '    # return the minimap given the raw position and orientations of the players\n'
+ '    # returns: (shape: (N, H, W, C)) \n'
+ '    # N: number of players to get_smm_state 5\n'
+ '    # H: height 47\n'
+ '    # W: width 87\n'
+ '    # C: channels 5\n'
+ '    SMM_WIDTH = 87\n'
+ '    SMM_HEIGHT = 47\n'
+ '    _MARKER_VALUE = 1\n'
+ '    # position is binary 0 or 1 \n'
+ '    # orientation is continuous [-1, 1]\n'
+ '    SMM_LAYERS = [\'our_team\', \'opp_team\', \'ball\', \'active_player\', \'active_player_orientation\']\n'
+ '    channel_dimensions = (SMM_WIDTH, SMM_HEIGHT)\n'
+ '    \n'
+ '    # initialize smm states\n'
+ '    states = np.zeros((len(cur_posture), channel_dimensions[1], channel_dimensions[0], len(SMM_LAYERS)))\n'
+ '    # first layer: our team\n'
+ '    for i in range(5):\n'
+ '        x = int((cur_posture[i][X] + field[X]/2 + goal[X])*10)\n'
+ '        y = int((cur_posture[i][Y] + field[Y]/2)*10)\n'
+ '        x = max(0, min(SMM_WIDTH - 1, x))\n'
+ '        y = max(0, min(SMM_HEIGHT - 1, y))\n'
+ '        states[i][y][x][3] = _MARKER_VALUE\n'
+ '        states[i][y][x][4] = cur_posture[i][TH]/math.pi\n'
+ '        for j in range(5):\n'
+ '            states[j][y][x][0] = _MARKER_VALUE\n'
+ '    # second layer: opponent team\n'
+ '    for i in range(5):\n'
+ '        x = int((cur_posture_opp[i][X] + field[X]/2 + goal[X])*10)\n'
+ '        y = int((cur_posture_opp[i][Y] + field[Y]/2)*10)\n'
+ '        x = max(0, min(SMM_WIDTH - 1, x))\n'
+ '        y = max(0, min(SMM_HEIGHT - 1, y))\n'
+ '        for j in range(5):\n'
+ '            states[j][y][x][1] = _MARKER_VALUE\n'
+ '    # third layer: ball\n'
+ '    x = int((cur_ball[X] + field[X]/2 + goal[X])*10)\n'
+ '    y = int((cur_ball[Y] + field[Y]/2)*10)\n'
+ '    x = max(0, min(SMM_WIDTH - 1, x))\n'
+ '    y = max(0, min(SMM_HEIGHT - 1, y))\n'
+ '    for j in range(5):\n'
+ '        states[j][y][x][2] = _MARKER_VALUE\n'
+ '\n'
+ '    return states\n'
+ '\n'
+ 'def get_reward(reward_type, cur_posture, prev_posture, cur_ball, prev_ball, field, id):\n'
+ '    \n'
+ '    if reward_type == "positive_followball":\n'
+ '        reward = get_positive_reward(cur_posture, prev_posture, cur_ball, prev_ball, field, id)\n'
+ '    if reward_type == "negative_followball":\n'
+ '        reward = get_negative_reward(cur_posture, prev_posture, cur_ball, prev_ball, field, id)\n'
+ '    if reward_type == "basic":\n'
+ '        reward = get_basic_reward(cur_posture, prev_posture, cur_ball, prev_ball, field, id)\n'
+ '    if reward_type == "differential":\n'
+ '        reward = get_differential_reward(cur_posture, prev_posture, cur_ball, prev_ball, field, id)\n'
+ '    if reward_type == "checkpoint":\n'
+ '        reward = get_checkpoint_reward(cur_posture, prev_posture, cur_ball, prev_ball, field, id)\n'
+ '\n'
+ '    return reward\n'
+ '\n'
+ 'def get_positive_reward(cur_posture, prev_posture, cur_ball, prev_ball, field, id):\n'
+ '    dist_robot2ball = helper.distance(cur_posture[id][X] , cur_ball[X], cur_posture[id][Y], cur_ball[Y])\n'
+ '    robot_th_error = abs(math.atan2(cur_ball[Y]-cur_posture[id][Y], cur_ball[X]-cur_posture[id][X]) - cur_posture[id][TH])\n'
+ '    ball_robot_dis = helper.distance(cur_posture[id][X] , cur_ball[X], cur_posture[id][Y], cur_ball[Y])\n'
+ '    ball_robot_dis_prev = helper.distance(prev_posture[id][X] , prev_ball[X], prev_posture[id][Y], prev_ball[Y])\n'
+ '    ball_robot_velocity = (ball_robot_dis_prev - ball_robot_dis)/0.05\n'
+ '\n'
+ '    return (0.5*(math.exp(-1*dist_robot2ball)) + 0.5*(math.exp(-1*robot_th_error)) + np.clip(1-math.exp(-1*(ball_robot_velocity)),0,1))\n'
+ '\n'
+ 'def get_negative_reward(cur_posture, prev_posture, cur_ball, prev_ball, field, id):\n'
+ '    dist_robot2ball = helper.distance(cur_posture[id][X] , cur_ball[X], cur_posture[id][Y], cur_ball[Y])\n'
+ '\n'
+ '    return -(1-math.exp(-1*dist_robot2ball))\n'
+ '\n'
+ 'def get_basic_reward(cur_posture, prev_posture, cur_ball, prev_ball, field, id):\n'
+ '    dist_robot2ball = helper.distance(cur_posture[id][X] , cur_ball[X], cur_posture[id][Y], cur_ball[Y])\n'
+ '    dist_ball2goal = helper.distance(field[X]/2 , cur_ball[X], 0, cur_ball[Y])\n'
+ '    score = 0\n'
+ '    concede = 0\n'
+ '    # need to get the score information from simulator variables\n'
+ '    if cur_ball[X] > field[X]/2 :\n'
+ '        score = 10\n'
+ '    if cur_ball[X] < -field[X]/2 :\n'
+ '        concede = -10  \n'
+ '\n'
+ '    return score + concede - (1-math.exp(-1*dist_robot2ball)) - (1-math.exp(-1*dist_ball2goal))\n'
+ '\n'
+ 'def get_differential_reward(cur_posture, prev_posture, cur_ball, prev_ball, field, id):\n'
+ '    dist_robot2ball = helper.distance(cur_posture[id][X] , cur_ball[X], cur_posture[id][Y], cur_ball[Y])\n'
+ '    prev_dist_robot2ball = helper.distance(prev_posture[id][X] , prev_ball[X], prev_posture[id][Y], prev_ball[Y])\n'
+ '    dist_ball2goal = helper.distance(field[X]/2 , cur_ball[X], 0, cur_ball[Y])\n'
+ '    prev_dist_ball2goal = helper.distance(field[X]/2 , prev_ball[X], 0, prev_ball[Y])\n'
+ '    delta_robot2ball = dist_robot2ball - prev_dist_robot2ball\n'
+ '    delta_ball2goal = dist_ball2goal - prev_dist_ball2goal\n'
+ '    score = 0\n'
+ '    concede = 0\n'
+ '    # need to get the score information from simulator variables\n'
+ '    if cur_ball[X] > field[X]/2 :\n'
+ '        score = 10\n'
+ '    if cur_ball[X] < -field[X]/2 :\n'
+ '        concede = -10  \n'
+ '\n'
+ '    return score + concede - delta_robot2ball - delta_ball2goal\n'
+ '\n'
+ 'def get_checkpoint_reward(cur_posture, prev_posture, cur_ball, prev_ball, field, id):\n'
+ '    cur_checkpoint = helper.ball_is_checkpoint(cur_ball, field)\n'
+ '    prev_checkpoint = helper.ball_is_checkpoint(prev_ball, field)\n'
+ '    score = 0\n'
+ '    concede = 0\n'
+ '    checkpoint_differential = 0\n'
+ '    if cur_checkpoint > prev_checkpoint:\n'
+ '        checkpoint_differential = 1\n'
+ '    elif cur_checkpoint > prev_checkpoint:\n'
+ '        checkpoint_differential = -1\n'
+ '        \n'
+ '    # need to get the score information from simulator variables\n'
+ '    if cur_ball[X] > field[X]/2 :\n'
+ '        score = 10\n'
+ '    if cur_ball[X] < -field[X]/2 :\n'
+ '        concede = -10 \n'
+ '\n'
+ '    return score + concede + checkpoint_differential\n'
+ '\n'
+ 'def get_action(action_type, robot_id, action_number, max_linear_velocity):\n'
+ '\n'
+ '    if action_type == "discrete":\n'
+ '        wheels = get_discrete_action(robot_id, action_number, max_linear_velocity)\n'
+ '    if action_type == "extendeddiscrete":\n'
+ '        wheels = get_extendeddiscrete_action(robot_id, action_number, max_linear_velocity)\n'
+ '    \n'
+ '    return wheels\n'
+ '\n'
+ 'def get_discrete_action(robot_id, action_number, max_linear_velocity):\n'
+ '    # 7 actions\n'
+ '    wheels = [0 for _ in range(30)]\n'
+ '\n'
+ '    wheels[6*robot_id + 0] = Discrete.WHEELS[action_number][0]*max_linear_velocity[robot_id]\n'
+ '    wheels[6*robot_id + 1] = Discrete.WHEELS[action_number][1]*max_linear_velocity[robot_id]\n'
+ '    wheels[6*robot_id + 2] = Discrete.WHEELS[action_number][2]\n'
+ '    wheels[6*robot_id + 3] = Discrete.WHEELS[action_number][3]\n'
+ '    wheels[6*robot_id + 4] = Discrete.WHEELS[action_number][4]\n'
+ '    wheels[6*robot_id + 5] = Discrete.WHEELS[action_number][5]\n'
+ '\n'
+ '    return wheels\n'
+ '\n'
+ 'def get_extendeddiscrete_action(robot_id, action_number, max_linear_velocity):\n'
+ '    # 13 actions\n'
+ '    wheels = [0 for _ in range(30)]\n'
+ '\n'
+ '    wheels[6*robot_id + 0] = ExtendedDiscrete.WHEELS[action_number][0]*max_linear_velocity[robot_id]\n'
+ '    wheels[6*robot_id + 1] = ExtendedDiscrete.WHEELS[action_number][1]*max_linear_velocity[robot_id]\n'
+ '    wheels[6*robot_id + 2] = ExtendedDiscrete.WHEELS[action_number][2]\n'
+ '    wheels[6*robot_id + 3] = ExtendedDiscrete.WHEELS[action_number][3]\n'
+ '    wheels[6*robot_id + 4] = ExtendedDiscrete.WHEELS[action_number][4]\n'
+ '    wheels[6*robot_id + 5] = ExtendedDiscrete.WHEELS[action_number][5]\n'
+ '\n'
+ '    return wheels\n'
+ '\n'
+ 'class Logger():\n'
+ '    def __init__(self):\n'
+ '\n'
+ '        self.episode = []\n'
+ '        self.m_episode = []\n'
+ '        self.value = []\n'
+ '        self.mean_value = []\n'
+ '\n'
+ '    def update(self, episode, value, num):\n'
+ '\n'
+ '        self.episode.append(episode)\n'
+ '        self.value.append(value)\n'
+ '        self.num = num\n'
+ '        if len(self.value) >= self.num :\n'
+ '            self.m_episode.append(episode - self.num/2)\n'
+ '            self.mean_value.append(np.mean(self.value[-self.num:]))\n'
+ '\n'
+ '    def plot(self, name):\n'
+ '        plt.title(str(name))\n'
+ '        plt.plot(self.episode, self.value, c = \'lightskyblue\', label=\'total_reward\') \n'
+ '        plt.plot(self.m_episode, self.mean_value, c = \'b\', label=\'Average_Total_Reward\') \n'
+ '        if len(self.episode) <= 10:\n'
+ '            plt.legend(loc=1)\n'
+ '        plt.savefig(\'../../examples/player_deeplearning-single-ddpg_py/TOTAL_\'+str(name)+\'.png\')\n';

var TRAIN_DDPG = 
'#!/usr/bin/python3\n'
+ '\n'
+ '# Author(s): Luiz Felipe Vecchietti, Kyujin Choi, Taeyoung Kim\n'
+ '# Maintainer: Kyujin Choi (nav3549@kaist.ac.kr)\n'
+ '\n'
+ 'import random\n'
+ 'import os\n'
+ 'import json\n'
+ 'import sys\n'
+ 'sys.path.append(os.path.dirname(os.path.realpath(__file__)) + \'/../common\')\n'
+ 'try:\n'
+ '    from participant import Participant, Game, Frame\n'
+ 'except ImportError as err:\n'
+ '    print(\'player_random-walk: "participant" module cannot be imported:\', err)\n'
+ '    raise\n'
+ '\n'
+ 'import math\n'
+ 'import numpy as np\n'
+ '\n'
+ 'import helper\n'
+ 'from ddpg import DDPG\n'
+ 'from rl_utils import get_reward, get_state, Logger\n'
+ '\n'
+ '#reset_reason\n'
+ 'NONE = Game.NONE\n'
+ 'GAME_START = Game.GAME_START\n'
+ 'SCORE_MYTEAM = Game.SCORE_MYTEAM\n'
+ 'SCORE_OPPONENT = Game.SCORE_OPPONENT\n'
+ 'GAME_END = Game.GAME_END\n'
+ 'DEADLOCK = Game.DEADLOCK\n'
+ 'GOALKICK = Game.GOALKICK\n'
+ 'CORNERKICK = Game.CORNERKICK\n'
+ 'PENALTYKICK = Game.PENALTYKICK\n'
+ 'HALFTIME = Game.HALFTIME\n'
+ 'EPISODE_END = Game.EPISODE_END\n'
+ '\n'
+ '#game_state\n'
+ 'STATE_DEFAULT = Game.STATE_DEFAULT\n'
+ 'STATE_KICKOFF = Game.STATE_KICKOFF\n'
+ 'STATE_GOALKICK = Game.GOALKICK\n'
+ 'STATE_CORNERKICK = Game.CORNERKICK\n'
+ 'STATE_PENALTYKICK = Game.STATE_PENALTYKICK\n'
+ '\n'
+ '#coordinates\n'
+ 'MY_TEAM = Frame.MY_TEAM\n'
+ 'OP_TEAM = Frame.OP_TEAM\n'
+ 'BALL = Frame.BALL\n'
+ 'X = Frame.X\n'
+ 'Y = Frame.Y\n'
+ 'Z = Frame.Z\n'
+ 'TH = Frame.TH\n'
+ 'ACTIVE = Frame.ACTIVE\n'
+ 'TOUCH = Frame.TOUCH\n'
+ 'BALL_POSSESSION = Frame.BALL_POSSESSION\n'
+ '\n'
+ '#robot_index\n'
+ 'GK_INDEX = 0 \n'
+ 'D1_INDEX = 1 \n'
+ 'D2_INDEX = 2 \n'
+ 'F1_INDEX = 3 \n'
+ 'F2_INDEX = 4\n'
+ '\n'
+ 'class Frame(object):\n'
+ '    def __init__(self):\n'
+ '        self.time = None\n'
+ '        self.score = None\n'
+ '        self.reset_reason = None\n'
+ '        self.game_state = None\n'
+ '        self.subimages = None\n'
+ '        self.coordinates = None\n'
+ '        self.half_passed = None\n'
+ '\n'
+ 'class Player(Participant):\n'
+ '    def init(self, info):\n'
+ '        params_file = open(os.path.dirname(__file__) + \'/parameters.json\')\n'
+ '        params = json.loads(params_file.read())\n'
+ '        self.field = info[\'field\']\n'
+ '        self.max_linear_velocity = info[\'max_linear_velocity\']\n'
+ '        self.goal = info[\'goal\']\n'
+ '        self.number_of_robots = info[\'number_of_robots\']\n'
+ '        self.end_of_frame = False\n'
+ '        self._frame = 0 \n'
+ '        self.wheels = [0 for _ in range(30)]\n'
+ '        self.cur_posture = []\n'
+ '        self.prev_posture = []\n'
+ '        self.cur_posture_opp = []\n'
+ '        self.cur_ball = []\n'
+ '        self.prev_ball = []\n'
+ '\n'
+ '        self.state_type = params[\'sim_parameters\'][\'state_type\']\n'
+ '        self.action_type = params[\'sim_parameters\'][\'action_type\']\n'
+ '        self.reward_type = params[\'sim_parameters\'][\'reward_type\']\n'
+ '\n'
+ '        self.previous_frame = Frame()\n'
+ '        self.frame_skip = params[\'sim_parameters\'][\'frame_skip\'] # number of frames to skip\n'
+ '        if ( self.state_type == \'relative\'):\n'
+ '            self.obs_size = 8 # state size\n'
+ '        elif ( self.state_type == \'full\'):\n'
+ '            self.obs_size = 37 # state size      \n'
+ '        if (self.action_type == \'discrete\'):\n'
+ '            self.act_size = 7 # number of discrete actions\n'
+ '        elif (self.action_type == \'extendeddiscrete\'):\n'
+ '            self.act_size = 13 # number of discrete actions \n'
+ '        elif (self.action_type == \'continuous\'):\n'
+ '            self.act_size = 2 # number of discrete actions\n'
+ '        self.robot_index = np.where(params[\'sim_parameters\'][\'robot\'])[0][0]\n'
+ '\n'
+ '        # for RL\n'
+ '        self.number_of_agents = 1 # in this example, just F2 is trained\n'
+ '        self.action = [0, 0]\n'
+ '        self.previous_action = [0, 0]\n'
+ '        self.state = []\n'
+ '        self.previous_state = []\n'
+ '        self.reward = 0\n'
+ '        self.previous_reward = 0\n'
+ '        self.terminal = False\n'
+ '\n'
+ '        # RL algorithm class\n'
+ '        if ( params[\'sim_parameters\'][\'algorithm\'] == \'ddpg\'):\n'
+ '            self.trainer = DDPG(self.obs_size, self.act_size)\n'
+ '        \n'
+ '        # log rewards\n'
+ '        self.total_reward = 0\n'
+ '        self.rew = np.zeros(4)\n'
+ '        # for logging rewards\n'
+ '        self.t = 0\n'
+ '        self.episode = 1\n'
+ '        self.plot_reward = Logger()\n'
+ '        self.save_png_interval = 10\n'
+ '        helper.printConsole("Initializing variables...")\n'
+ '\n'
+ '    def get_coord(self, received_frame):\n'
+ '        self.cur_ball = received_frame.coordinates[BALL]\n'
+ '        self.cur_posture = received_frame.coordinates[MY_TEAM]\n'
+ '        self.cur_posture_opp = received_frame.coordinates[OP_TEAM]\n'
+ '        self.prev_posture = self.previous_frame.coordinates[MY_TEAM]\n'
+ '        self.prev_posture_opp = self.previous_frame.coordinates[OP_TEAM]\n'
+ '        self.prev_ball = self.previous_frame.coordinates[BALL]\n'
+ '\n'
+ '    def update(self, received_frame):\n'
+ '\n'
+ '        if received_frame.end_of_frame:\n'
+ '        \n'
+ '            self._frame += 1\n'
+ '\n'
+ '            if (self._frame == 1):\n'
+ '                self.previous_frame = received_frame\n'
+ '                self.get_coord(received_frame)\n'
+ '\n'
+ '            self.get_coord(received_frame)\n'
+ '\n'
+ '            if self._frame % self.frame_skip == 1:\n'
+ '        \n'
+ '                # Get reward and state\n'
+ '                self.reward = get_reward(self.reward_type, self.cur_posture, self.prev_posture, self.cur_ball, self.prev_ball, self.field, self.robot_index)\n'
+ '                state = get_state(self.state_type, self.cur_posture, self.prev_posture, self.cur_posture_opp, self.prev_posture_opp, self.cur_ball, self.prev_ball, self.field, self.goal, self.max_linear_velocity) \n'
+ '                \n'
+ '                # select next action / only training for F2 robot\n'
+ '                self.state = np.reshape([state[self.robot_index]],(1, self.obs_size))\n'
+ '                self.action = self.trainer.select_action(self.state)\n'
+ '                self.terminal = 0.0 if received_frame.reset_reason is not NONE else 1.0\n'
+ '                self.total_reward += self.reward\n'
+ '                self.t += 1\n'
+ '\n'
+ '                # just store experiences if the F2 robot is active\n'
+ '                if self.cur_posture[self.robot_index][ACTIVE]:\n'
+ '                    if self._frame == 1:\n'
+ '                        self.trainer.store_experience(self.state, self.state, self.previous_action, self.previous_reward, self.terminal)\n'
+ '                    else:\n'
+ '                        self.trainer.store_experience(self.previous_state, self.state, self.previous_action, self.previous_reward, self.terminal)\n'
+ '\n'
+ '            else:\n'
+ '                self.action = self.previous_action\n'
+ '\n'
+ '            # Set wheel speeds and send to the simulator  \n'
+ '            self.wheels[6*self.robot_index + 0] = self.action[0][0] * self.max_linear_velocity[self.robot_index]\n'
+ '            self.wheels[6*self.robot_index + 1] = self.action[0][1] * self.max_linear_velocity[self.robot_index]\n'
+ '            self.set_speeds(self.wheels)\n'
+ '\n'
+ '            # Training script: called every timestep  \n'
+ '            self.trainer.update()\n'
+ '                \n'
+ '            # save checkpoint\n'
+ '            self.trainer.save_checkpoint(self._frame)\n'
+ '            \n'
+ '            # logging training agents reward and plot graph \n'
+ '            if (received_frame.reset_reason > 1) :\n'
+ '\n'
+ '                if self.t >= 10:\n'
+ '                    mean_total_reward = self.total_reward/(self.t)\n'
+ '                    self.plot_reward.update(self.episode, mean_total_reward, 5)\n'
+ '                    if self.episode % self.save_png_interval == 0:\n'
+ '                        self.plot_reward.plot(\'DQN-AGENT-REWARD\') \n'
+ '                    self.episode += 1\n'
+ '                # reset episode timesteps and total reward \n'
+ '                self.t = 0\n'
+ '                self.total_reward = 0\n'
+ '\n'
+ '            # save to update the replay buffer in the next state\n'
+ '            self.previous_state = self.state\n'
+ '            self.previous_action = self.action\n'
+ '            self.previous_reward = self.reward\n'
+ '\n'
+ '            self.end_of_frame = False\n'
+ '            self.previous_frame = received_frame\n'
+ '\n'
+ 'if __name__ == \'__main__\':\n'
+ '    player = Player()\n'
+ '    player.run()\n';

var PLAY_DDPG = 
'#!/usr/bin/python3\n'
+ '\n'
+ '# Author(s): Luiz Felipe Vecchietti, Kyujin Choi, Taeyoung Kim\n'
+ '# Maintainer: Kyujin Choi (nav3549@kaist.ac.kr)\n'
+ '\n'
+ 'import random\n'
+ 'import os\n'
+ 'import json\n'
+ 'import sys\n'
+ 'sys.path.append(os.path.dirname(os.path.realpath(__file__)) + \'/../common\')\n'
+ 'try:\n'
+ '    from participant import Participant, Game, Frame\n'
+ 'except ImportError as err:\n'
+ '    print(\'player_random-walk: "participant" module cannot be imported:\', err)\n'
+ '    raise\n'
+ '\n'
+ 'import sys\n'
+ '\n'
+ 'import math\n'
+ 'import numpy as np\n'
+ '\n'
+ 'import helper\n'
+ 'from ddpg import DDPG\n'
+ 'from rl_utils import  get_state\n'
+ '\n'
+ '#reset_reason\n'
+ 'NONE = Game.NONE\n'
+ 'GAME_START = Game.GAME_START\n'
+ 'SCORE_MYTEAM = Game.SCORE_MYTEAM\n'
+ 'SCORE_OPPONENT = Game.SCORE_OPPONENT\n'
+ 'GAME_END = Game.GAME_END\n'
+ 'DEADLOCK = Game.DEADLOCK\n'
+ 'GOALKICK = Game.GOALKICK\n'
+ 'CORNERKICK = Game.CORNERKICK\n'
+ 'PENALTYKICK = Game.PENALTYKICK\n'
+ 'HALFTIME = Game.HALFTIME\n'
+ 'EPISODE_END = Game.EPISODE_END\n'
+ '\n'
+ '#game_state\n'
+ 'STATE_DEFAULT = Game.STATE_DEFAULT\n'
+ 'STATE_KICKOFF = Game.STATE_KICKOFF\n'
+ 'STATE_GOALKICK = Game.GOALKICK\n'
+ 'STATE_CORNERKICK = Game.CORNERKICK\n'
+ 'STATE_PENALTYKICK = Game.STATE_PENALTYKICK\n'
+ '\n'
+ '#coordinates\n'
+ 'MY_TEAM = Frame.MY_TEAM\n'
+ 'OP_TEAM = Frame.OP_TEAM\n'
+ 'BALL = Frame.BALL\n'
+ 'X = Frame.X\n'
+ 'Y = Frame.Y\n'
+ 'Z = Frame.Z\n'
+ 'TH = Frame.TH\n'
+ 'ACTIVE = Frame.ACTIVE\n'
+ 'TOUCH = Frame.TOUCH\n'
+ 'BALL_POSSESSION = Frame.BALL_POSSESSION\n'
+ '\n'
+ '#robot_index\n'
+ 'GK_INDEX = 0 \n'
+ 'D1_INDEX = 1 \n'
+ 'D2_INDEX = 2 \n'
+ 'F1_INDEX = 3 \n'
+ 'F2_INDEX = 4\n'
+ '\n'
+ 'class Frame(object):\n'
+ '    def __init__(self):\n'
+ '        self.time = None\n'
+ '        self.score = None\n'
+ '        self.reset_reason = None\n'
+ '        self.game_state = None\n'
+ '        self.subimages = None\n'
+ '        self.coordinates = None\n'
+ '        self.half_passed = None\n'
+ '\n'
+ 'class Player(Participant):\n'
+ '    def init(self, info):\n'
+ '        params_file = open(os.path.dirname(__file__) + \'/parameters.json\')\n'
+ '        params = json.loads(params_file.read())\n'
+ '        self.field = info[\'field\']\n'
+ '        self.max_linear_velocity = info[\'max_linear_velocity\']\n'
+ '        self.goal = info[\'goal\']\n'
+ '        self.number_of_robots = info[\'number_of_robots\']\n'
+ '        self.end_of_frame = False\n'
+ '        self._frame = 0 \n'
+ '        self.wheels = [0 for _ in range(30)]\n'
+ '        self.cur_posture = []\n'
+ '        self.prev_posture = []\n'
+ '        self.cur_posture_opp = []\n'
+ '        self.cur_ball = []\n'
+ '        self.prev_ball = []\n'
+ '\n'
+ '        self.state_type = params[\'sim_parameters\'][\'state_type\']\n'
+ '        self.action_type = params[\'sim_parameters\'][\'action_type\']\n'
+ '\n'
+ '        self.previous_frame = Frame()\n'
+ '        self.frame_skip = params[\'sim_parameters\'][\'frame_skip\'] # number of frames to skip\n'
+ '        if (self.state_type == \'relative\'):\n'
+ '            self.obs_size = 8 # state size\n'
+ '        elif (self.state_type == \'full\'):\n'
+ '            self.obs_size = 37 # state size \n'
+ '        if (self.action_type == \'discrete\'):\n'
+ '            self.act_size = 7 # number of discrete actions\n'
+ '        elif (self.action_type == \'extendeddiscrete\'):\n'
+ '            self.act_size = 13 # number of discrete actions \n'
+ '        elif (self.action_type == \'continuous\'):\n'
+ '            self.act_size = 2 # number of discrete actions\n'
+ '        self.robot_index = np.where(params[\'sim_parameters\'][\'robot\'])[0][0]\n'
+ '\n'
+ '        # for RL\n'
+ '        self.action = 0\n'
+ '        self.previous_action = 0\n'
+ '\n'
+ '        self.num_inputs = self.obs_size\n'
+ '        # RL algorithm class\n'
+ '        self.load = True\n'
+ '        self.play = True\n'
+ '        self.trainer = DDPG(self.obs_size, self.act_size, self.load, self.play)\n'
+ '\n'
+ '        helper.printConsole("Initializing variables...")\n'
+ '\n'
+ '    def get_coord(self, received_frame):\n'
+ '        self.cur_ball = received_frame.coordinates[BALL]\n'
+ '        self.cur_posture = received_frame.coordinates[MY_TEAM]\n'
+ '        self.cur_posture_opp = received_frame.coordinates[OP_TEAM]\n'
+ '        self.prev_posture = self.previous_frame.coordinates[MY_TEAM]\n'
+ '        self.prev_posture_opp = self.previous_frame.coordinates[OP_TEAM]\n'
+ '        self.prev_ball = self.previous_frame.coordinates[BALL]\n'
+ '\n'
+ '    def update(self, received_frame):\n'
+ '\n'
+ '        if received_frame.end_of_frame:\n'
+ '        \n'
+ '            self._frame += 1\n'
+ '\n'
+ '            if (self._frame == 1):\n'
+ '                self.previous_frame = received_frame\n'
+ '                self.get_coord(received_frame)\n'
+ '\n'
+ '            self.get_coord(received_frame)\n'
+ '\n'
+ '            if self._frame % self.frame_skip == 1:\n'
+ '        \n'
+ '                # Get reward and state\n'
+ '                state = get_state(self.state_type, self.cur_posture, self.prev_posture, self.cur_posture_opp, self.prev_posture_opp, self.cur_ball, self.prev_ball, self.field, self.goal, self.max_linear_velocity) \n'
+ '\n'
+ '                # select next action / only training for F2 robot\n'
+ '                self.state = np.reshape([state[self.robot_index]],(1, self.obs_size))\n'
+ '                self.action = self.trainer.select_action(self.state)               \n'
+ '\n'
+ '            else:\n'
+ '                self.action = self.previous_action\n'
+ '\n'
+ '            # Set wheel speeds and send to the simulator\n'
+ '            self.wheels[6*self.robot_index + 0] = self.action[0][0] * self.max_linear_velocity[self.robot_index]\n'
+ '            self.wheels[6*self.robot_index + 1] = self.action[0][1] * self.max_linear_velocity[self.robot_index]\n'
+ '            self.set_speeds(self.wheels)\n'
+ '\n'
+ '            self.end_of_frame = False\n'
+ '            self.previous_action = self.action\n'
+ '            self.previous_frame = received_frame\n'
+ '\n'
+ 'if __name__ == \'__main__\':\n'
+ '    player = Player()\n'
+ '    player.run()\n';